<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Stan Zonov</title>
    <description></description>
    <link>https://szonov.com/</link>
    <atom:link href="https://szonov.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 13 Jul 2022 01:46:48 -0500</pubDate>
    <lastBuildDate>Wed, 13 Jul 2022 01:46:48 -0500</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>Hot Peppers</title>
        <description>&lt;p&gt;I have created another video. Please go &lt;a href=&quot;https://www.youtube.com/watch?v=atcKhoJ6vCc&quot;&gt;here&lt;/a&gt; to see the video! This time it is about my experimentation with growing hot pepper plants indoors during the winter. These plants provided me with inspiration, joy and a taste of spring. I am very grateful for the experience and look forward to growing more stuff this upcoming winter. Summer is so short here that I am already preparing.&lt;/p&gt;

&lt;p&gt;I am starting to get the hang of making videos and will likely make more. I like the creativity involved and the opportunity to film non programming/technical stuff. Maybe next time I will make a video of me playing something on the piano. Stay tuned!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/hot_peppers.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 12 Jul 2022 19:00:00 -0500</pubDate>
        <link>https://szonov.com/planting/2022/07/12/hot-peppers/</link>
        <guid isPermaLink="true">https://szonov.com/planting/2022/07/12/hot-peppers/</guid>
        
        
        <category>planting</category>
        
      </item>
    
      <item>
        <title>Train Pictures</title>
        <description>&lt;p&gt;Hello dear reader!&lt;/p&gt;

&lt;p&gt;I have created a video of a project I have been working on for a while. In addition to being a dear reader you can now be a dear viewer too! I am very excited to share this with you.&lt;/p&gt;

&lt;p&gt;The video is about an automatic mechanism I created for capturing pictures of trains that go across a bridge in view of my apartment.&lt;/p&gt;

&lt;p&gt;Please go &lt;a href=&quot;https://www.youtube.com/watch?v=PlVF_0MooQE&quot;&gt;here&lt;/a&gt; to see the video!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/photos/winnipeg/IMG_6151.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the video I reference code that I have written as part of the project. The code is all discussed in my previous few blog posts:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://szonov.com/programming/2021/03/28/prototype/&quot;&gt;Prototype&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://szonov.com/programming/2021/08/15/controlling-dslr-using-android-app-raspberry-pi-and-gphoto2/&quot;&gt;Controlling DSLR using Android App, Raspberry Pi and gphoto2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://szonov.com/programming/2021/08/30/simple-online-realtime-tracking/&quot;&gt;Simple Online Realtime Tracker&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://szonov.com/programming/2021/11/05/detecting-coins-with-tensorflow/&quot;&gt;Detecting Coins with Tensorflow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://szonov.com/programming/2021/11/12/tensorflow-lite-speed-comparison/&quot;&gt;TensorFlow Lite Speed Comparison&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://szonov.com/programming/2022/01/03/react-image-tagging/&quot;&gt;React Image Tagging&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Wed, 02 Feb 2022 14:00:00 -0600</pubDate>
        <link>https://szonov.com/programming/2022/02/02/train-pictures/</link>
        <guid isPermaLink="true">https://szonov.com/programming/2022/02/02/train-pictures/</guid>
        
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>React Image Tagging</title>
        <description>&lt;p&gt;Hello dear reader!&lt;/p&gt;

&lt;p&gt;I created an in-browser image tagging tool using React and Python/Flask for the backend. The code is &lt;a href=&quot;https://github.com/mannyray/ImageTagger&quot;&gt;here&lt;/a&gt; along with a very detailed explanation in the README.md.&lt;/p&gt;

&lt;p&gt;The tool can be used for tagging many pictures with ease and then later for searching for the images via tags. Here are some sample screenshots:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/react/manitoba.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/react/search_2.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It was great to learn React. It was unsual and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;setState&lt;/code&gt; command at times seemed like the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;goto&lt;/code&gt; command that I always thought was improper to use. It took some time to adjust. I am grateful for the learning experience.&lt;/p&gt;
</description>
        <pubDate>Mon, 03 Jan 2022 11:00:00 -0600</pubDate>
        <link>https://szonov.com/programming/2022/01/03/react-image-tagging/</link>
        <guid isPermaLink="true">https://szonov.com/programming/2022/01/03/react-image-tagging/</guid>
        
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>TensorFlow Lite Speed Comparison</title>
        <description>&lt;p&gt;Hello dear reader!&lt;/p&gt;

&lt;p&gt;In this blog post we will investigate the speed of TensorFlow Lite models on various Android phones. TensorFlow Lite models are efficient and portable TensorFlow models that are used in machine learning applications on mobile devices like Android phones. In our case we will investigate the models from previous &lt;a href=&quot;https://szonov.com/programming/2021/11/05/detecting-coins-with-tensorflow/&quot;&gt;blog post&lt;/a&gt; where the models generated can detect coins (see screenshot of app in action below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/detecting_coins/result.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We will test two TensorFlow Lite models&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz&lt;/code&gt;&lt;/a&gt; pretrained Tensorflow 2 model which was trained further and converted to Tflite format in &lt;a href=&quot;https://github.com/mannyray/machineLearningStarter/blob/master/run_tensorflow/result_2.tflite&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;result_2.tflite&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md#mobile-models&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssd_mobilenet_v3_large_coco_2020_01_14.tar.gz&lt;/code&gt;&lt;/a&gt; pretrained Tensorflow 1 model which was trained further and converted to Tflite format in &lt;a href=&quot;https://github.com/mannyray/machineLearningStarter/blob/master/run_tensorflow/result.tflite&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;result.tflite&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The TensorFlow Lite models are tested on four different Android phones&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gsmarena.com/samsung_galaxy_j2_prime-8424.php&quot;&gt;Samsung Galaxy J2 Prime&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;1.5 GB Ram&lt;/li&gt;
      &lt;li&gt;2016 November Release&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gsmarena.com/motorola_one_action-9739.php&quot;&gt;Motorola One Action&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;4 GB Ram&lt;/li&gt;
      &lt;li&gt;2019 October Release&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gsmarena.com/google_pixel_4-9896.php&quot;&gt;Google Pixel 4&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;6 GB Ram&lt;/li&gt;
      &lt;li&gt;2019 October Release&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gsmarena.com/google_pixel_6-11037.php&quot;&gt;Google Pixel 6&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;8 GB Ram&lt;/li&gt;
      &lt;li&gt;2021 October Release&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-are-we-investigating&quot;&gt;What are we investigating?&lt;/h2&gt;

&lt;p&gt;We want to test the performance of the TensorFlow Lite models on the various Android phones. When running, the TensorFlow Lite model for input takes an image/snapshot from your cellphone’s camera and processes it and then outputs coordinates of where the detected objects are (in our case these objects are coins). The coordinates are used to plot bounding boxes around the objects as seen in screenshot from cellphone above.&lt;/p&gt;

&lt;p&gt;The input output happens over and over again to produce real time object detection. The time to process an image is measured in milliseconds and will be the measure of performance. The speed of processing depends on the TensorFlow model as well as the cellphone’s hardware. The faster the processing time the better the performance.&lt;/p&gt;

&lt;p&gt;We will investigate the two TensorFlow Lite models on the four phones to see how they perform. Finally, we want to investigate how to improve performance (i.e. processing time of an image with TensorFlow Lite model) for a fixed phone.&lt;/p&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;p&gt;Here we will briefly discuss the testing procedure used.&lt;/p&gt;

&lt;p&gt;The app for benchmark is from &lt;a href=&quot;https://github.com/mannyray/tfliteCustomApp/tree/benchmark_app&quot;&gt;tfliteCustomApp repository&lt;/a&gt;. To download locally:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/mannyray/tfliteCustomApp.git
cd tfliteCustomApp
git checkout benchmark_app
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Connect your Android phone to Android Studio and build and then run the app on your phone. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;result.tflite&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;result_2.tflite&lt;/code&gt; models can be placed in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tfliteCustomApp/app/src/main/assets&lt;/code&gt; directory by replacing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;detect.tflite&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;Make the appropriate modifications (if necessary) to the app in Android Studio and build and run. Usually we are either modifying the exact model used, modifying the thread count or adding a fan to the phone. Your context may vary.&lt;/p&gt;

&lt;p&gt;We set the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Clear log before launch&lt;/code&gt; to enabled in between each run. Run the android app to start logging results.&lt;/p&gt;

&lt;p&gt;You can filter for the exact processing times by filter specific &lt;a href=&quot;https://github.com/mannyray/tfliteCustomApp/blob/benchmark_app/app/src/main/java/org/tensorflow/lite/benchmark/detection/DetectorActivity.java#L225&quot;&gt;log line&lt;/a&gt; in Android Studio:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/tensorflow_lite_comparison/log.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The phone will be connected via USB cable to computer in order to collect logs. The phone will be detecting same objects for duration of your experiment runtime. Once finishing running your app you can copy the logs to a file for later processing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/tensorflow_lite_comparison/coins.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;We will start with looking at a specific cell phone model (Motorola One Action) and look at TensorFlow Lite model performance (the one trained with TensorFlow 1).&lt;/p&gt;

&lt;p&gt;We introduce a new parameter, thread count, that can be specified in the app during runtime. We experiment with varying thread count to determine the best performance possible for a given cell phone as the performance varies with thread count. In the figure below we run the app on the phone for thirty seconds. The first 10 seconds are cut off as that is when the application is loading or one is modifying the thread count via UI and so the performance is not stable.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/tensorflow_lite_comparison/Figure_1.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From the figure we can observe that the performance varies significantly when varying thread count. In the case of Motorola One Action running the TensorFlow 2 model, we have the red line, representing four thread count, is the best performing configuration with average processing time of 75ms. When we decrease OR increase thread count from four we increase the processing time. For low and high thread count (1 or 6) we observe the variance in processing time has increased compared to the optimal thread count of four.&lt;/p&gt;

&lt;p&gt;The previous image was specifically for Motorola One Action. What happens if we fix a TensorFlow Model and see how the various Android models perform? In the image below we plot the average one minute performance of each of the four cell phones on the TensorFlow 1 trained model while we vary the thread count.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/tensorflow_lite_comparison/Figure_2.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The old Samsung J2 prime is so slow that we cannot really see how well the others are performing. We can tell that the other phones are much better with the Pixels being the best and near each other performance wise. Let us see what happens when we normalize each of the curves in the figure above based on its maximum value (we normalize each curve by dividing each value by the largest value present in the curve which reduces the values to range of zero to one):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/tensorflow_lite_comparison/Figure_3.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The maximum values are either with one thread count or six as that is where the cell phone is the most slowest. For all the cell phones we can tell that the optimal performance is somewhere in between one and six threads. For Motorola One Action and Pixel 4 we observe that the optimal time is twice as good as the worst processing time (0.5 ratio). We observe that some phones are optimal at three threads while others at four (Pixel 4 vs Motorola One Action).&lt;/p&gt;

&lt;p&gt;With the previous figure we lose the ability to compare performance of a cell phone to another cell phone. Let us compare the fastest processing time (among all thread counts) for all cell phones against each model. The processing time is the average of one minute.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;TensorFlow 1 Model&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;TensorFlow 2 Model&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Samsung J2 Prime&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;517ms&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1174ms&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Motorola One Action&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;75ms&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;216ms&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Pixel 4&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;39ms&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;102ms&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Pixel 6&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;41ms&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;82ms&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We observe that the TensorFlow 2 model is slower for all the cell phones by at least a factor of two. We observe that Pixel 6 is the fastest but comparable to Pixel 4 when using TensorFlow 1 model ( 39ms to 41ms too close say one is better than the other).&lt;/p&gt;

&lt;p&gt;It is important to note that I do not mean that TensorFlow 1 is better than TensorFlow 2. When I use “TensorFlow N model” I am referring to with which version the model was trained with. The models themselves are different which you can confirm yourself by analyzing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.tflite&lt;/code&gt;s in &lt;a href=&quot;https://netron.app&quot;&gt;https://netron.app/&lt;/a&gt;. Due to TensorFlow versioning and based on the work &lt;a href=&quot;https://github.com/mannyray/machineLearningStarter&quot;&gt;here&lt;/a&gt; each model can only be created in that specific version of TensorFlow. The TensorFlow 1 model is more lightweight so it is expected that it performs better. Lightweight models can be found in TensorFlow 2 as well.&lt;/p&gt;

&lt;p&gt;When using some other model for your application you will have to consider its speed and understand that it can have a significant impact on performance as demonstrated in the table above. You will also have to to consider its ability of correctly detecting objects. I happened to train these specific models as these were the first ones I was able to setup the training data to TensorFlow Lite model pipeline successfully. TensorFlow pipeline can be a pain to setup.&lt;/p&gt;

&lt;p&gt;In addition, you will also have to consider the phone model. In our study the Samsung J2 Prime was the slowest phone and not competitive to others. What if we compare the Motorola One Action and Pixel 4 for the TensorFlow 1 Model. The former is 75ms while the latter is 39ms in average processing time. Is 75ms sufficient for your specific application?&lt;/p&gt;

&lt;p&gt;Let us assume your application is for tracking an object as it moves across a screen as it was in my &lt;a href=&quot;https://szonov.com/programming/2021/03/28/prototype/&quot;&gt;application&lt;/a&gt;. For the purpose of tracking I implemented a &lt;a href=&quot;https://github.com/mannyray/sort/&quot;&gt;Simple Online Realtime Tracking&lt;/a&gt; algorithm which is built on top of a Kalman Filter. In tracking applications, we need to make sure we have an accurate estimate of where our object is located. Our estimate of where the object is located depends on how frequently we obtain our measurements (among other things). For TensorFlow Lite models, the measurements are the bounding boxes around detected objects. In the video below observe the difference in lag of the bounding box as the thread count is increased and thus processing time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=xOvVSYoBjm8&quot; title=&quot;Tensorflow tflite app lag example&quot;&gt;&lt;img src=&quot;https://szonov.com/assets/tensorflow_lite_comparison/coin_video.png&quot; alt=&quot;IMAGE ALT TEXT&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;An extreme example of tracking lag is if you are tracking an ant on the ground but you can only check once a minute. Last time you saw the ant, he was heading north. In a minute you expect him to be slightly north of where you last saw him. What if the ant turned west half way through the one minute? Now you have to search the ground for the ant. If you were checking once every 10 seconds you wouldn’t have to spend as much time looking for him even if he changed direction. Frequency of measurements is relevant to your ability to track. If we double the processing time from one phone model to the next (or from one TensorFlow model to the next) then this can have significant impact on you as your measurements are less frequent.&lt;/p&gt;

&lt;p&gt;In tracking, a cost function of accuracy is the Intersection Over Union measure. In the gif below consider the red is the true position of an object and blue box where you think it is. As the blue box more closely matches the location of the red, the IOU increases to a maximum of one. IOU is zero when there is no intersection.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/mannyray/sort/raw/master/python_implementation/example/assets/iou_example.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Some stills from the gif above:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://szonov.com/assets/tensorflow_lite_comparison/target-160.png&quot; alt=&quot;img&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://szonov.com/assets/tensorflow_lite_comparison/target-175.png&quot; alt=&quot;img&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://szonov.com/assets/tensorflow_lite_comparison/target-188.png&quot; alt=&quot;img&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://szonov.com/assets/tensorflow_lite_comparison/target-200.png&quot; alt=&quot;img&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In the image below I plot the IOU as I increase the time difference between arriving measurements in a simulation I ran &lt;a href=&quot;https://github.com/mannyray/sort/tree/master/python_implementation/example&quot;&gt;here&lt;/a&gt; (see example 13). The increase in time between arriving measurements could represent increased processing time for TensorFlow model. Too much lag can significantly reduce IOU (see black line and red line). The stills from gif can help get an idea of significance of lower IOU.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/tensorflow_lite_comparison/lag_analysis.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can always tune the Kalman filter or Simple Online Realtime Tracking algorithm to account for slower processing time but it can be annoying especially if you are not familiar with the algorithm. The faster the TensorFlow model the less you need to rely on your Kalman filter. For examples on tuning please see my work &lt;a href=&quot;https://github.com/mannyray/sort/tree/master/python_implementation/example&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;So far we have investigated processing time over a small period of time of thirty seconds. For tracking applications (or others) you may want to run your app for much longer. Let us see what happens when we increase runtime to fifteen minutes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/tensorflow_lite_comparison/Figure_4.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this case we are running the Pixel 4 with TensorFlow 2 model. The blue line is us running with a cooling fan while the orange is just the phone on its own. Images of the fan are below. It attaches to the phone directly.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://szonov.com/assets/tensorflow_lite_comparison/cooler_1.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://szonov.com/assets/tensorflow_lite_comparison/cooler_2.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;With the fan, the average processing time is around 90ms while without the average is at first 110ms and then increases to around 130ms. Without the fan, the phone gets really hot and overtime can shutdown from overheating.&lt;/p&gt;

&lt;p&gt;Adding a fan is one quick way to improve your application’s performance in the long run.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this blog post I analysed the performance of TensorFlow Lite models on various Android phones and provided real world numbers. I briefly discussed TensorFlow Lite models in the context of object tracking applications. Finally, I provided a good tip for long term TensorFlow Lite execution on the phone: using a fan.&lt;/p&gt;

&lt;p&gt;The applications of TensorFlow Lite models seem endless to me. I am very grateful that TensorFlow has provided the technology for developing these models and has provided sample code for which I can easily get started in creating these applications like in the case of the benchmark app. A recent application in which I used some of the information presented here can be found in a previous post about a &lt;a href=&quot;https://szonov.com/programming/2021/03/28/prototype/&quot;&gt;prototype&lt;/a&gt;. The gif of the prototype, that tracks oranges, is below. I hope the reader finds the information presented here useful for their experiments too.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/prototype/out.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 12 Nov 2021 11:00:00 -0600</pubDate>
        <link>https://szonov.com/programming/2021/11/12/tensorflow-lite-speed-comparison/</link>
        <guid isPermaLink="true">https://szonov.com/programming/2021/11/12/tensorflow-lite-speed-comparison/</guid>
        
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>Detecting Coins with Tensorflow</title>
        <description>&lt;p&gt;Hello dear reader! I am very excited to share with you an Android app that can detect coins.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/detecting_coins/canada_coins.JPG&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/detecting_coins/result.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is a demonstration video of the app in action (click the image):&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.youtube.com/watch?v=w_0esVCmKus&quot; title=&quot;Video Title&quot;&gt;&lt;img src=&quot;http://img.youtube.com/vi/w_0esVCmKus/0.jpg&quot; alt=&quot;IMAGE ALT TEXT&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The repository for the code can be found &lt;a href=&quot;https://github.com/mannyray/machineLearningStarter&quot;&gt;here&lt;/a&gt;. The repository is meant to provide a series of scripts and instructions on how to get started with Tensorflow tflite models on the Android.&lt;/p&gt;
</description>
        <pubDate>Fri, 05 Nov 2021 17:00:00 -0500</pubDate>
        <link>https://szonov.com/programming/2021/11/05/detecting-coins-with-tensorflow/</link>
        <guid isPermaLink="true">https://szonov.com/programming/2021/11/05/detecting-coins-with-tensorflow/</guid>
        
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>Simple Online Realtime Tracker</title>
        <description>&lt;p&gt;Hello dear reader! I come with another post about code related to my &lt;a href=&quot;https://szonov.com/programming/2021/03/28/prototype/&quot;&gt;prototype&lt;/a&gt; project. The &lt;a href=&quot;https://szonov.com/programming/2021/08/15/controlling-dslr-using-android-app-raspberry-pi-and-gphoto2/&quot;&gt;last post&lt;/a&gt; showcased code where an Android App was able to control a DSLR camera via Bluetooth with a Raspberry Pi’s assistance. The Android App would send a capture image signal to the DSLR upon user’s manual input on the Android App. For object detection applications we want to automate this based on some object’s position.&lt;/p&gt;

&lt;p&gt;For my prototype, I would take a high resolution picture with the DSLR when the object was detected in a specific range. In the gif below, this range would be the yellow box and the objects are oranges.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/prototype/out.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When a new orange enters the range box (scope) the Android app would send a take picture signal. In the image below is when the signal is taken.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/prototype/video_capture.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And here is the picture taken by the DSLR (prototype setup so value of picture not important):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/prototype/IMG_6378.JPG&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;how-does-one-track-an-object-as-it-moves-around-the-view-of-the-androids-camera&quot;&gt;How does one track an object as it moves around the view of the Android’s camera?&lt;/h3&gt;

&lt;p&gt;For this I used the Simple Online Realtime Tracker (SORT) algorithm based on&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Bewley, Alex, et al. &quot;Simple online and realtime tracking.&quot; 2016 IEEE international conference on image processing (ICIP). IEEE, 2016.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The SORT combines the Kalman Filter and Hungarian algorithm together to keep track of the bounding boxes an image detecting machine learning model would output for each frame of some video. Every frame the machine learning model tells you that &lt;em&gt;here&lt;/em&gt; is an orange and &lt;em&gt;there&lt;/em&gt; is an orange and &lt;em&gt;over there&lt;/em&gt;. However, the image detecting machine learning model will not tell you that an orange in one frame at location X is the same orange at location Y. The SORT solves this problem.&lt;/p&gt;

&lt;p&gt;I have created a Python implementation and then translated it to Java since that is the language used in Android App development. Developing and testing the algorithm in Python is easier. The code can be found &lt;a href=&quot;https://github.com/mannyray/sort&quot;&gt;here&lt;/a&gt;. To get started with using the code go to&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/mannyray/sort/tree/master/python_implementation&quot;&gt;https://github.com/mannyray/sort/tree/master/python_implementation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/mannyray/sort/tree/master/java_implementation&quot;&gt;https://github.com/mannyray/sort/tree/master/java_implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For a detailed explanation of SORT algorithm and various ways you could tune its parameters please go to&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/mannyray/sort/tree/master/python_implementation/example&quot;&gt;https://github.com/mannyray/sort/tree/master/python_implementation/example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The motiviation for creating this standalone SORT library was&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Not wanting to learn/use someone else’s code and discover its limitations a little to late into the implementation&lt;/li&gt;
  &lt;li&gt;Freedom to easily customize and modify my code based on my project’s needs&lt;/li&gt;
  &lt;li&gt;I already had experience with the &lt;a href=&quot;https://github.com/mannyray/kalmanfilter&quot;&gt;Kalman Filter&lt;/a&gt; which is half of the SORT algorithm&lt;/li&gt;
  &lt;li&gt;Learn more about object detection type code&lt;/li&gt;
  &lt;li&gt;Create something someone else might use&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are a reader who is looking for a SORT algorihtm, I hope you find mine useful!&lt;/p&gt;
</description>
        <pubDate>Mon, 30 Aug 2021 17:00:00 -0500</pubDate>
        <link>https://szonov.com/programming/2021/08/30/simple-online-realtime-tracking/</link>
        <guid isPermaLink="true">https://szonov.com/programming/2021/08/30/simple-online-realtime-tracking/</guid>
        
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>Controlling DSLR using Android App, Raspberry Pi and gphoto2</title>
        <description>&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Hello dear reader! In this post I will be discussing how to control a DSLR camera using an Android app, Raspberry Pi, gphoto2 and some &lt;a href=&quot;https://github.com/mannyray/bluetoothPhoto&quot;&gt;code&lt;/a&gt;. I found this capability very useful for my endeavours and hope that you may as well for yours. I will go over motivations, how to setup, linked code and an example use case. If you are restless, at the end I have a gif of a time-lapse I was able to create with the help of this app.&lt;/p&gt;

&lt;p&gt;Here is a view of the deployed system:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/schema.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;gphoto2 is a free application that can be used to control a variety of &lt;a href=&quot;http://www.gphoto.org/proj/libgphoto2/support.php&quot;&gt;different cameras&lt;/a&gt; via commandline.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Raspberry Pi can be replaced with another computer. Raspberry Pi is chosen as it is leightweight and cheap.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Android app is useful because it allows for wireless, remote control and a nice user interface.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;here-are-reasons-why-you-may-find-the-setup-useful&quot;&gt;Here are reasons why you may find the setup useful:&lt;/h2&gt;

&lt;h3 id=&quot;cameras-come-with-limitations&quot;&gt;Cameras come with limitations&lt;/h3&gt;

&lt;p&gt;In this blog I will be using the Canon M100.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/m100.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the case of the M100, the camera has a limitation on how long you can take time-lapse photos for. In the image below, three screenshots from the M100 configuration screen show that you can set to take photos every 2 to 4 seconds and up to 900 shots. Combining the two upper limits you can only takes a time-lapse spanning an hour in time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/camera_settings.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What if you want to take a time-lapse of a full day? With the use of a battery cable and this setup you can overcome such limitations. The Android acts as user interface enhancement instead of using gphoto2 commandline directly on the Raspberry Pi.&lt;/p&gt;

&lt;p&gt;M100 battery cable:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/wired_battery.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-camera-manufacturer-provided-android-app-is-not-sufficient&quot;&gt;The camera manufacturer provided Android app is not sufficient&lt;/h3&gt;

&lt;p&gt;The Android app provided here can be modified to monitor for events for which regular manufacture provided Android apps for your camera will not have. Want to take picture at a specific time? Take a picture only when you detect something in the field of focus of cell phone camera? Take a picture when you say “action”? You will have to code this yourself for which the Android app introduced in this post provides a great starting point.&lt;/p&gt;

&lt;p&gt;The programmatic freedom allows one to integrate your camera into a variety of applications.&lt;/p&gt;

&lt;h3 id=&quot;broken-camera-screen&quot;&gt;Broken camera screen&lt;/h3&gt;

&lt;p&gt;May sound like a stretch but I had this happen. Camera menu screen was busted. The app can be useful here for providing a basic screen interface.&lt;/p&gt;

&lt;h2 id=&quot;how-to-setup&quot;&gt;How to Setup&lt;/h2&gt;

&lt;h3 id=&quot;install-gphoto2-on-raspberry-pi&quot;&gt;Install gphoto2 on Raspberry Pi&lt;/h3&gt;

&lt;p&gt;You will have to install gphoto2 software. &lt;a href=&quot;https://github.com/gonzalo/gphoto2-updater&quot;&gt;This&lt;/a&gt; is a good repository on how to do this.&lt;/p&gt;

&lt;h3 id=&quot;connect-the-camera-to-the-raspberry-pi&quot;&gt;Connect the Camera to the Raspberry Pi&lt;/h3&gt;

&lt;p&gt;Connect the camera to the Raspberry Pi. In the case of the M100, I use the USB to mini USB B. Cable below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/wire.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The mini USB B port on the M100 is pictured below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/port.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The result will look something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/connection.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Turn your camera on and see if your camera has a setting to stay on without sleeping. On the M100 the setting looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/never_off.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the Raspberry Pi terminal run&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ps aux | grep gphoto
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kill&lt;/code&gt; on the non &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt; results. Next run&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gphoto2 --shell
&amp;gt;trigger-capture
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/initial_connection.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sometimes taking pictures will fail. In the image above, the failure was due to camera not focusing while in auto focus mode. For the second picture, I had to make sure I had better lighting.&lt;/p&gt;

&lt;p&gt;gphoto2 supports many &lt;a href=&quot;http://www.gphoto.org/proj/libgphoto2/support.php&quot;&gt;cameras&lt;/a&gt; to varying degrees. Your debug process and figuring out how to run your camera will thus vary. Be patient. By searching your error on a search engine you can usually find a thread on gphoto2’s repository or some stackoverflow post discussing a potential workaround. Here are some links that I found useful:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/gphoto/libgphoto2/issues/521&quot;&gt;maybe you are using an older version of gphoto2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/gphoto/libgphoto2/issues/30&quot;&gt;use debug mode&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/gphoto/libgphoto2/issues/197&quot;&gt;saving to sd card&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;…happy debugging!&lt;/p&gt;

&lt;h3 id=&quot;connecting-raspberry-pi-to-android-phone&quot;&gt;Connecting Raspberry Pi to Android Phone&lt;/h3&gt;

&lt;p&gt;Connect your phone and Raspberry Pi together. There are many instructions online on how to do this like &lt;a href=&quot;https://bluedot.readthedocs.io/en/latest/pairpiandroid.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;setting-up-the-bluetooth-server-on-raspberry-pi&quot;&gt;Setting up the Bluetooth server on Raspberry Pi&lt;/h3&gt;

&lt;p&gt;On the Raspberry Pi you will have to run&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo python server.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The python script can be found &lt;a href=&quot;https://github.com/mannyray/bluetoothPhoto/tree/master/bluetooth_server&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Apply a setting to your Raspberry Pi so it does not fall asleep and stays on.&lt;/p&gt;

&lt;p&gt;If you experience errors: make sure to install the required packages. For potential debugging when failing to advertise I found &lt;a href=&quot;https://stackoverflow.com/questions/37913796/bluetooth-error-no-advertisable-device&quot;&gt;this&lt;/a&gt; to be helpful.&lt;/p&gt;

&lt;h3 id=&quot;build-the-android-app&quot;&gt;Build the Android app&lt;/h3&gt;

&lt;p&gt;You must be able to build the Android app using Android Studio. This can get a bit involved for a first time builder. I recommend starting &lt;a href=&quot;https://developer.android.com/training/basics/firstapp&quot;&gt;here&lt;/a&gt; to get introduced to the basics.&lt;/p&gt;

&lt;p&gt;The location of the app code is &lt;a href=&quot;https://github.com/mannyray/bluetoothPhoto/tree/master/bluetooth_android_app/Bluetooth&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;intermission&quot;&gt;Intermission&lt;/h3&gt;

&lt;p&gt;If you followed instructions above you now have&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Bluetooth-paired the Android phone and Raspberry Pi&lt;/li&gt;
  &lt;li&gt;Launched the Bluetooth server on the Raspberry Pi&lt;/li&gt;
  &lt;li&gt;Connected your camera to the Raspberry Pi&lt;/li&gt;
  &lt;li&gt;Built and installed the application on your Android.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For illustration purposes I have setup the Raspberry Pi and camera.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/test_setup.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The camera is connected to the Raspberry Pi and the battery is cable powered.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/camera_test_setup.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The test subject image here is a map of Winnipeg:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/camera_test_subject.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Bluetooth server on the Raspberry Pi is running and waiting for a connection.&lt;/p&gt;

&lt;h3 id=&quot;running-the-android-phone-app&quot;&gt;Running the Android Phone App&lt;/h3&gt;

&lt;p&gt;Now let us launch the Android application which is called “Bluetooth”. The landing page will look like&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/landing_page.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is me holding the phone:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/view_of_phone.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After pressing connect you will be redicted to&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/choose_page.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And the Raspberry Pi will accept the connection:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/bluetooth_server_waiting.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When selecting “Single picture” and pressing the camera button you will see:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/picture_taken.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Bluetooth server will reflect these pictures taken:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/bluetooth_server_picture_taken.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The picture is also saved to the SD card of the camera.&lt;/p&gt;

&lt;p&gt;The other option here is to take a time-lapse:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/time_lapse.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The “Shutdown Signal” is for when you want to disconnect and quit the Bluetooth server. The app will then redict you back to the landing page.&lt;/p&gt;

&lt;h2 id=&quot;time-lapse-example&quot;&gt;Time-lapse example&lt;/h2&gt;

&lt;p&gt;Here is an example of the time-lapse feature. After the pictures were taken I combined them to form a gif. Earlier, I suggested that the M100’s time-lapse feature could be extended for a whole day. I was a bit lazy to do that for this post and did something that is quicker and would take up less memory:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/dlsr_control/sand.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now the skeptical reader might ask: “Why do you need an Android app to send a single command to start executing a time-lapse or even a single photograph? Why not modify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;server.py&lt;/code&gt; and run the code straight on the Raspberry Pi?”&lt;/p&gt;

&lt;p&gt;It is true. In such cases the phone is unnecessary. I could just run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gphoto2 --shell&lt;/code&gt; in the terminal. However, for applications where you need more advance features, the Android phone is very versatile. In this &lt;a href=&quot;https://szonov.com/programming/2021/03/28/prototype/&quot;&gt;post&lt;/a&gt; I describe such an advanced application that builds on top of the logic introduced here by using the phone’s camera and a Tensorflow model. This post is supposed to provide starting code and general introduction to combining the phone, camera and Raspberry Pi together. Happy hacking!&lt;/p&gt;
</description>
        <pubDate>Sun, 15 Aug 2021 07:00:00 -0500</pubDate>
        <link>https://szonov.com/programming/2021/08/15/controlling-dslr-using-android-app-raspberry-pi-and-gphoto2/</link>
        <guid isPermaLink="true">https://szonov.com/programming/2021/08/15/controlling-dslr-using-android-app-raspberry-pi-and-gphoto2/</guid>
        
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>Prototype</title>
        <description>&lt;p&gt;I am very excited to show you a prototype of a project I have been working on. The project involves using an image detection cell phone application to detect “interesting events” and acting upon these events. An “interesting event” will be application specific. For example some cell phones use the &lt;a href=&quot;https://www.youtube.com/watch?v=ZwCNG9KFdXs&quot;&gt;front facing camera to detect their owner’s face as a way of authentication&lt;/a&gt;. In this example the “interesting event” is the owner’s face. The action taken is unlocking the phone.&lt;/p&gt;

&lt;p&gt;In my application, I want the cell phone’s image detection application to detect a specific object type. Upon detection, the cell phone will request a DSLR camera to take a high resolution picture of this object. This object will be around two hundred meters away: far enough that the cell phone’s camera is insufficient to get a detailed view. Here is a diagram of my project:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/prototype/scheme.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the previous diagram, additional implementation details are exposed. The Raspberry Pi, which can be replaced with some other computer, is necessary because there is no easy and direct way to send a signal from one’s cellphone to a regular DSLR camera. New DSLR cameras often come with downloadable applications from the app store. However, these application are insufficient in providing detailed control over the DSLR for your custom application. The &lt;a href=&quot;http://gphoto.org/&quot;&gt;gphoto&lt;/a&gt; applications allows one to resolve this problem.&lt;/p&gt;

&lt;p&gt;gphoto allows one to send commands via USB cable from a computer to your DSLR camera. A bluetooth server was implemented and then installed on the Raspberry Pi to act as the middle man between the cell phone application and the DSLR. When the cell phone application detects the far away object, it sends a Bluetooth signal to the Raspberry Pi. Then, the Raspberry Pi sends the “take picture” command to the DSLR.&lt;/p&gt;

&lt;p&gt;For my prototype, the far away objects will be oranges. I simulated this using a projector and a wall.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/prototype/IMG_6809.JPG&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/prototype/IMG_6808.JPG&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A script was written to simulate the oranges moving in the background. The script outputs a gif. Here is a small bit of the output:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/prototype/oranges.gif&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The cellphone with the detection application was placed with the oranges in view:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/prototype/IMG_6813.JPG&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the above image, the cellphone application detects the oranges and displays a bounding box around each detected one. The middle yellow box acts as an aiming scope. When an orange is fully within the yellow box, the cellphone application will send the bluetooth signal for the DSLR to take the picture. The DSLR is setup so the direction it points to aligns with where the yellow box ‘aim scope’ is pointing to.&lt;/p&gt;

&lt;p&gt;The application was built on top of a sample application provided by &lt;a href=&quot;https://www.tensorflow.org/lite&quot;&gt;TensorFlow&lt;/a&gt;. My most significant upgrade for this application is adding my custom Simple Online and Realtime Tracking (SORT) algorithm described in &lt;a href=&quot;https://arxiv.org/abs/1602.00763&quot;&gt;this paper&lt;/a&gt;. SORT allows the cellphone application to keep track of a trajectory of detected objects. The trajectory is used for identifying when a new orange has entered the yellow box.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/prototype/IMG_6810.JPG&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The camera is setup on a tripod with two cables sticking out. Top left one connects the DSLR to the Raspberry Pi while the bottom right connects the camera battery to an outlet. Using an outlet connected battery over a regular battery allows for you to run the camera over long periods of time.&lt;/p&gt;

&lt;p&gt;A gif of the app running is below&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/prototype/out.gif&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The incoming oranges are detected by the application and are bounded with a box with specified identification tag. When an orange with an id that has not yet been in the yellow box comes into the yellow box, the cellphone application sends a signal for the DSLR to take a picture. A dimming/brightening feature was added to brighten when the take picture signal is sent.&lt;/p&gt;

&lt;p&gt;From the gif, a frame is extracted where the take picture signal was sent (based on brightness). At &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2:16.440&lt;/code&gt;, with the center of the orange numbered &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;42&lt;/code&gt; directly above &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:&lt;/code&gt; of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2:16&lt;/code&gt;, the cellphone application sends the signal:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/prototype/video_capture.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The DSLR lags by half a second and takes the picture when the orange numbered &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;42&lt;/code&gt; is centered above the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;02&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/prototype/IMG_6378.JPG&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Fine level of accuracy was not the level of focus for this prototype testing. The delay for my application is sufficient. However, limitations of the camera are important to note. For this experiment, the Canon EOS M100 was used. If lighting is poor, the DSLR will reject the “take picture” command. At times the camera will need to refocus (if on autofocus mode) which will cause a bit of lag with the “take picture” command.&lt;/p&gt;

&lt;p&gt;The simulation lasted a total of 2 and a half minutes with a total of 48 objects detected and matching pictures taken. Pictures took almost 500Mb of memory as a high resolution option was set for the captured images.&lt;/p&gt;

&lt;p&gt;To apply this to something other than oranges, a new detection model must be created. This can be thought of as a new game cartridge to a game boy advance. The gaming experience (in this application) is dictated by the cartridge (in this case detection model). In addition, instead of sending a signal to a DSLR camera you can send the signal to some other device depending on what you are trying to do.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I am satisfied with this prototype. It is ready for further testing. Most of the learning required for this prototype was android application development and the SORT algorithm. The code for these I will share later. The learning occupied a lot of time this past half year and I am grateful to see results.&lt;/p&gt;

&lt;p&gt;For amateur purposes, this prototype can most easily be setup in one’s place of living as you need a secure place to leave your camera to run. This means that to take interesting pictures, you need to have an interesting view out your window. Once you have a subject area of focus and a trained image detecting model, you can apply set this prototype pretty easily. To some this prototype may seem like a fishing net in the desert - not very useful. However, if you are by the ocean then have fun!&lt;/p&gt;

&lt;p&gt;The initial inspiration to create this prototype came from &lt;a href=&quot;https://www.cbc.ca/news/canada/toronto/councillor-ana-bailao-vacancy-tax-jaco-joubert-condo-tower-study-1.5357198&quot;&gt;this article&lt;/a&gt;. Toronto at the time had a housing problem (probably does and will). Housing is scarce and ever increasing in price. A developer wanted to determine what percentage of condo units are unoccupied and unused in a city where people are desperate for housing. He setup a camera and started monitoring condo towers from afar so he could see which ones are occupied or not. After reading the article, I would look out my window and see what information I can observe and absorb with my camera.&lt;/p&gt;

&lt;p&gt;The next big step for my prototype is to train a new image detecting model for what I want to observe and absorb outside my window. Will not spoil the details of what exactly here.&lt;/p&gt;
</description>
        <pubDate>Sun, 28 Mar 2021 16:00:00 -0500</pubDate>
        <link>https://szonov.com/programming/2021/03/28/prototype/</link>
        <guid isPermaLink="true">https://szonov.com/programming/2021/03/28/prototype/</guid>
        
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>First project</title>
        <description>&lt;p&gt;Apologies - I have not posted for over a year. During this time, I have been busy working on a couple of projects that are not yet ready to share.&lt;/p&gt;

&lt;p&gt;Last summer, I wrote a guide on how to setup a &lt;a href=&quot;https://github.com/mannyray/AWScommentSection&quot;&gt;comment section&lt;/a&gt; hosted on AWS as part of learning more about AWS’ DynamoDB and Lambda. I enjoyed creating the comment section. I decided against adding it to this site since I do not get much traffic here.&lt;/p&gt;

&lt;p&gt;Found the output of my final project for my grade 10 computer science course (eleven years ago I think). The animation is supposed to be a Star Wars like scene. I remember being especially proud of the space battle between the two groups of ships as it looked suprisingly good. What was your first project?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/starwars.gif&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Fri, 26 Feb 2021 07:00:00 -0600</pubDate>
        <link>https://szonov.com/programming/2021/02/26/first-project/</link>
        <guid isPermaLink="true">https://szonov.com/programming/2021/02/26/first-project/</guid>
        
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>A little bit of SLAM</title>
        <description>&lt;h1 id=&quot;a-little-bit-of-slam&quot;&gt;A little bit of SLAM&lt;/h1&gt;

&lt;p&gt;Let’s work with SLAM. A great series of papers on the topic are here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Simultaneous Localisation and Mapping (SLAM):Part I The Essential Algorithms - Tim Bailey and Hugh Durrant-Whyte&lt;/li&gt;
  &lt;li&gt;Simultaneous Localisation and Mapping (SLAM):Part II State of the Art - Tim Bailey and Hugh Durrant-Whyte&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Simultaneous Localization and Mapping (SLAM) “problem asks if it is possible for a mobile robot to be placed at an unknown location in an unknown environment and for the robot to incrementally build a consistent map of this environment while simultaneously determining its location within this map.”&lt;/p&gt;

&lt;p&gt;Let’s get our hands dirty right away and formulate a system to work with. We will be working with a 2D system where our robot along with landmarks will be treated as (x,y) points. To keep things simple, our robot will be moving on a predefined path such that it does not collide with the landmarks to avoid adding additional logic to the robot. We will be working with Octave.&lt;/p&gt;

&lt;p&gt;We first define the robot starting conditions as well as the landmarks:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-matlab&quot; data-lang=&quot;matlab&quot;&gt;&lt;span class=&quot;nb&quot;&gt;clear&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;close&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;%set the seed - one of the few commands used here that is _unique_ to Octave&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;seed&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;%robot starting initial conditions&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;robotX&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;robotY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;spreadFromCenter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xMin&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spreadFromCenter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xMax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spreadFromCenter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;yMin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spreadFromCenter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;yMax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spreadFromCenter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;%landmarks on the map&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xCoords&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;80&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;90&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;110&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;120&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;80&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;90&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;110&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;120&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;yCoords&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;80&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;90&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;110&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;120&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;90&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;120&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;130&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pointNumber&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xCoords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Let’s setup the robot’s movement, before worrying about the landmarks. Assume the robot is on a rail road - the y coordinate is unaffected. The robot will be moving at a constant speed with white noise affecting the motion. The following image will describe the situation:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-matlab&quot; data-lang=&quot;matlab&quot;&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;hold&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xCoords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yCoords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'*b'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;robotX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;robotY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'or'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'filled'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;robotX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;robotX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;robotY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'k--'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Landmarks'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Robot'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Robot path'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spreadFromCenter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spreadFromCenter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'x'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;equal&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/SLAM/setup1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now lets generate an actual instance of the robot’s motion.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-matlab&quot; data-lang=&quot;matlab&quot;&gt;&lt;span class=&quot;c1&quot;&gt;%total timesteps of robot&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;timesteps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;%speed of the robot (distance covered by timestep)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;speed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;%function representing the movement - first entry is x coordinate, second y.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;next_func&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coord&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;coord&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;speed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt; 
&lt;span class=&quot;c1&quot;&gt;%process noise - a little noise is added to model in order to be able to do Cholesky decomposition&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;processNoiseCovariance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.00000000001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;%store the results&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;movementCoords&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timesteps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;%initial conditions&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;movementCoords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;robotX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;movementCoords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;robotY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timesteps&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;movementCoords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movementCoords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;chol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;processNoiseCovariance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;'*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; 
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now, let’s plot the y-coordinate evolution:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-matlab&quot; data-lang=&quot;matlab&quot;&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timesteps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movementCoords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Time'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;equal&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/SLAM/setup2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We will set it so that the robot has a maximum measurement radius. Read &lt;a href=&quot;https://www.cs.utexas.edu/~kuipers/slides/L17-FastSLAM.pdf&quot;&gt;this&lt;/a&gt; for more info on the measurement function used. We will generate noisy measurements of the landmarks based on the position of the landmark.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-matlab&quot; data-lang=&quot;matlab&quot;&gt;&lt;span class=&quot;n&quot;&gt;measurementRadius&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;angles&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timesteps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pointNumber&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;distances&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timesteps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pointNumber&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; 
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timesteps&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;curRobotX&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;movementCoords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;curRobotY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;movementCoords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;	

	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pointNumber&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;curRobotX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;curRobotY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xCoords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yCoords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]);&lt;/span&gt; 
		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;measurementRadius&lt;/span&gt;
			&lt;span class=&quot;nb&quot;&gt;distances&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;  
			&lt;span class=&quot;n&quot;&gt;angles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;atan2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yCoords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;curRobotY&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xCoords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;curRobotX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;	
	&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;%add noise&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;angleCovariance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.004&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;distanceCovariance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;anglesNoisy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;angles&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;angles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;angleCovariance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;distancesNoisy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;distances&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;distances&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distanceCovariance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now that we have measurements we are ready to code up our robot exploring the environment. We will assume that the robot does not know now anything about the landmarks:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-matlab&quot; data-lang=&quot;matlab&quot;&gt;&lt;span class=&quot;n&quot;&gt;landMarkDiscoveredCount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We will set the following variable to be used in identifying if a measurement is associated with a new landmark. If we already are aware that there are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; landmarks then we can compare the current measurement’s distance from the estimated location of the other &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; landmarks to decide if the new measurement is far enough to belong to a new landmark.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-matlab&quot; data-lang=&quot;matlab&quot;&gt;&lt;span class=&quot;n&quot;&gt;minMahalanobisDistance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The rest of the script is given below. In it the robot moves forward and once it is at the appropriate measurement range, it ‘receives’ measurements from the landmark (the distance and angle with respect to the robot). The robot then decides if it is associated with a new landmark or not. If yes, then it is registered and a new estimate and associated covariance is set up for the landmark’s Kalman filter. Otherwise, the Kalman filter (specifically the update phase) is used to update the estimate (that is closest to the measured landmark).&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-matlab&quot; data-lang=&quot;matlab&quot;&gt;&lt;span class=&quot;c1&quot;&gt;%estimate location of discovered landmarks&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;landMarkMeans&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{};&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;%estimate of angle and distance from robot to landmark&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;landMarkAngleDistances&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{};&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;%associated covariance with landMarkAngleDIstances&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;landMarkCovariances&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{};&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;%record of measurements &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;landMarkMeasurements&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{};&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;%record of all estimates&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;landMarkEstimates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{};&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;%creating animation to store images &lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;%can be a little slow in octave - remove saveas part &lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;%to just view it&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;hold&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'equal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;xlim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spreadFromCenter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;spreadFromCenter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timesteps&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;%current position of robot - here it is known &lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;%next extension would let the position be unknown &lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;%and the predict phase of the Kalman filter to deal with it&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;curRobotX&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;movementCoords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;curRobotY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;movementCoords&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;	
	
	&lt;span class=&quot;c1&quot;&gt;%there are a total pointNumber landmarks - which our robot has no idea.&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;%However, we still have to cycle through all the landmarks at a given timestep&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;%in order to simulate an incoming measurement based on measurementRadius value.&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pointNumber&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;distances&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;%-1 flag used to signify that landmark is not in range&lt;/span&gt;
			&lt;span class=&quot;c1&quot;&gt;%landmark in measurementRadius range&lt;/span&gt;
			&lt;span class=&quot;c1&quot;&gt;%convert the measured angle and distance to an X and Y position&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;landmarkX&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;curRobotX&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distancesNoisy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;anglesNoisy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt; 
			&lt;span class=&quot;n&quot;&gt;landmarkY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;curRobotY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distancesNoisy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;anglesNoisy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;

			&lt;span class=&quot;c1&quot;&gt;%variables to figure if landmark is new and if not then to &lt;/span&gt;
			&lt;span class=&quot;c1&quot;&gt;%which recorded one does it associate with&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;landMarkIndex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;isNew&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
			
			&lt;span class=&quot;c1&quot;&gt;%compute distance between measured landmark and all other estimates&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;minIndex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;minVal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; 
			&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkDiscoveredCount&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;mahalanobisDistance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landmarkX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landmarkY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;landMarkMeans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;'&lt;/span&gt;
					&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landmarkX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landmarkY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;landMarkMeans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}));&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minVal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;minVal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mahalanobisDistance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;elseif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minVal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mahalanobisDistance&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;minVal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mahalanobisDistance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;minIndex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
	
			
			&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minVal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minVal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minMahalanobisDistance&lt;/span&gt;
				&lt;span class=&quot;c1&quot;&gt;%-1 for first index&lt;/span&gt;
				&lt;span class=&quot;c1&quot;&gt;%minVal &amp;gt; ... means that it is far away from all previous measurements&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;isNew&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;landMarkDiscoveredCount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;landMarkDiscoveredCount&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;landMarkIndex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;landMarkDiscoveredCount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; 
			&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
				&lt;span class=&quot;c1&quot;&gt;%measurement is associated with estimate that already exists &lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;landMarkIndex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

			&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;isNew&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;
				&lt;span class=&quot;c1&quot;&gt;%run Kalman filter's update phase &lt;/span&gt;
				
				&lt;span class=&quot;c1&quot;&gt;%to define current estimate we need to compute the angle and distance keeping &lt;/span&gt;
				&lt;span class=&quot;c1&quot;&gt;%in mind that the robot has moved since we computed the estimate last (we are updating&lt;/span&gt;
				&lt;span class=&quot;c1&quot;&gt;%the estimate before running the update phase) &lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;previous_position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;landMarkMeans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;C_measurement&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;curRobotX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;curRobotY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;previous_position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;previous_position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]);&lt;/span&gt;
					&lt;span class=&quot;nb&quot;&gt;atan2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;previous_position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;curRobotY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;previous_position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;curRobotX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)];&lt;/span&gt;
				&lt;span class=&quot;c1&quot;&gt;%our update phase deals in [distance;angle] format&lt;/span&gt;
				&lt;span class=&quot;c1&quot;&gt;%which is converted in the previous calculation meaning&lt;/span&gt;
				&lt;span class=&quot;c1&quot;&gt;%that we can set jacobian to the identity matrix&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;C_jacobian&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;eye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
				

				&lt;span class=&quot;c1&quot;&gt;%run the update phase&lt;/span&gt;
				&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;estimate_next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;covariance_sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ddekf_update_phase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;...&lt;/span&gt;
					&lt;span class=&quot;nb&quot;&gt;chol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distanceCovariance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;angleCovariance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;...&lt;/span&gt;
					&lt;span class=&quot;nb&quot;&gt;chol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkCovariances&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;...&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;C_jacobian&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C_measurement&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C_measurement&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;...&lt;/span&gt;
					&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distancesNoisy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;anglesNoisy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]);&lt;/span&gt;
					
				&lt;span class=&quot;c1&quot;&gt;%store computed results&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;landMarkAngleDistances&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;estimate_next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;landMarkCovariances&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;covariance_sqrt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;covariance_sqrt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;landMarkMeans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;curRobotX&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;estimate_next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;estimate_next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;...&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;curRobotY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;estimate_next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;estimate_next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))];&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;landMarkMeasurements&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landmarkX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landmarkY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;landMarkEstimates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkMeans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}];&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
				&lt;span class=&quot;c1&quot;&gt;%in case landmark is new, then record results and intialise estimate and covariance &lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;landMarkMeans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landmarkX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landmarkY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;landMarkAngleDistances&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distancesNoisy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;anglesNoisy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)];&lt;/span&gt; 
				&lt;span class=&quot;n&quot;&gt;landMarkCovariances&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distanceCovariance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;angleCovariance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;landMarkIndex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;landMarkDiscoveredCount&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;landMarkMeasurements&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landmarkX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landmarkY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;landMarkEstimates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkMeans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}];&lt;/span&gt;	
			&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;%plot results&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jj&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkDiscoveredCount&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkMeasurements&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;ttmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;landMarkMeasurements&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
			&lt;span class=&quot;nb&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ttmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ttmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'.'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;%scatter(xCoords,yCoords,'b','*'); %actual location of the landmarks&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;f1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;drawCircle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;curRobotX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;curRobotY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurementRadius&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'g'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;f2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;curRobotX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;curRobotY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'filled'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;f3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;curRobotX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;curRobotX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;curRobotY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'k--'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	
	&lt;span class=&quot;n&quot;&gt;tmpPlot&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[];&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jj&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;landMarkDiscoveredCount&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;ttmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;landMarkMeans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;tmpPlot&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmpPlot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ttmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ttmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;k&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)];&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;%%%REMOVE saveas line for speedup&lt;/span&gt;
	&lt;span class=&quot;nb&quot;&gt;saveas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;strcat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'output'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;num2str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'%04d'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;nb&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;nb&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;nb&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;jj&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmpPlot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;nb&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmpPlot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
	
	&lt;span class=&quot;nb&quot;&gt;pause&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Gif below shows the robot discovering the landmarks. Black circles are the computed estimates while the red dots are the measurements of the landmarks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/SLAM/outputName.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;How close were the estimates? The blue stars are the true landmark locations, red dots are measurements throughout and black circles are the computed final estimates.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/SLAM/setup3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you want to see the code used here then see &lt;a href=&quot;https://szonov.com/assets/SLAM/script.m&quot;&gt;script.m&lt;/a&gt; and assist &lt;a href=&quot;ddekf_update_phase.m&quot;&gt;file&lt;/a&gt;. Also check out this &lt;a href=&quot;https://github.com/randvoorhies/SimpleSLAM&quot;&gt;repository&lt;/a&gt; that was inspiration for this little project.&lt;/p&gt;

</description>
        <pubDate>Sun, 01 Dec 2019 07:00:00 -0600</pubDate>
        <link>https://szonov.com/programming/2019/12/01/a-little-bit-of-slam/</link>
        <guid isPermaLink="true">https://szonov.com/programming/2019/12/01/a-little-bit-of-slam/</guid>
        
        
        <category>programming</category>
        
      </item>
    
  </channel>
</rss>
