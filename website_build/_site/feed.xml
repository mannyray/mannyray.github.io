<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Stan Zonov</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 16 Oct 2024 15:57:34 -0500</pubDate>
    <lastBuildDate>Wed, 16 Oct 2024 15:57:34 -0500</lastBuildDate>
    <generator>Jekyll v4.2.2</generator>
    
      <item>
        <title>What is leadership?</title>
        <description>&lt;p&gt;Hello everyone!&lt;/p&gt;

&lt;p&gt;I thought I would try something new with social media: I am going to go on a mission to figure out what true leadership is. I will do this by going on a bicycle touring trip for about a month and ask people along the way for advice and post it on LinkedIn via videos - we will learn together and together we will get better! I figured if someone wants to be a leader, then they should ask as many people as possible as to what a leader should be - it is important to remember that anyone might have something cool and new to say. Furthermore, it is my belief that any stranger you meet could be an angel in disguise so you must always be polite to all.&lt;/p&gt;

&lt;p&gt;I’ve done something like this before and wrote about it in &lt;a href=&quot;https://twotired.ca&quot;&gt;https://twotired.ca&lt;/a&gt;. There, I learned the concept of what a mission is and it prepared me for success in my missions that followed it. Worrying about the destination or logistics of a mission should be avoided at all costs and that you must take your mission day by day and all of a sudden you will arrive. Patience and laughter is key as is forgetting that you were on a mission to begin with. Your mind must be blank; God will take care of the rest. This is why I’m not sure where I’m going for my bike ride - you must stay tuned to find out!&lt;/p&gt;

&lt;p&gt;For this mission like with most of my missions, I don’t need too much stuff. The material weighs you down: you must be light, agile, and efficient - ready to go the distance in order to dominate: I believe this is the true mark of any professional. Even professional homeless people know this; the ones that carry less are usually more successful in their missions (shopping cart carrying ones always look miserable!).&lt;/p&gt;

&lt;p&gt;To start packing for a ride you do need to think of some basics like how many days you want before major pitstops  - this is set by the amount of clean underwear you have - then you need to find a laundry place and a day to rest those muscles. All other items just come into place. In the image below (feel free to open it in a new tab for a closer zoom!), you will see what I am bringing for my bike ride:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Prayer book and icons! God makes me a super professional.&lt;/li&gt;
  &lt;li&gt;Tobacco and tea to zone out in the rainy and melancholy times&lt;/li&gt;
  &lt;li&gt;Headphones to listen to music and YouTube videos  - it helps me try out new dance moves and play with my acting and singing skills! I love to sing!&lt;/li&gt;
  &lt;li&gt;Chocolate, because I heard some war pilots in the past carried some under their seat to survive if they had to parachute. Dark chocolate has a lot of fat.&lt;/li&gt;
  &lt;li&gt;Colourful clothes and swagger (not pictured) to impress and set the stage for conversations, and have fun with anyone you meet&lt;/li&gt;
  &lt;li&gt;A hat to protect from the sun and to look real professional - like an officer! I believe a hat must be earned!&lt;/li&gt;
  &lt;li&gt;First aid kit: just in case. Have a tick key as well - really don’t like ticks as they are creepy, alien looking and acting! They spread disease.&lt;/li&gt;
  &lt;li&gt;Tools for the bike&lt;/li&gt;
  &lt;li&gt;Clean underwear and socks (duh!)&lt;/li&gt;
  &lt;li&gt;Sleeping stuff like sleeping bag, pad and bivy tent&lt;/li&gt;
  &lt;li&gt;Bright Rain coat&lt;/li&gt;
  &lt;li&gt;A whistle - keeps the wild animals, in case they attack, scared!&lt;/li&gt;
  &lt;li&gt;Passsport&lt;/li&gt;
  &lt;li&gt;A head light to shine in on the darkness!&lt;/li&gt;
  &lt;li&gt;Honey - super good and healing!&lt;/li&gt;
  &lt;li&gt;Wool clothing for being warm!&lt;/li&gt;
  &lt;li&gt;Bike jersey from my randonneur club (have to represent and connect with local clubs - it’s all an international thing you must know)&lt;/li&gt;
  &lt;li&gt;Bike lights and batteries - safety first (fun is a close second!)&lt;/li&gt;
  &lt;li&gt;Water bottles for when I am in between cities on a hot sunny day&lt;/li&gt;
  &lt;li&gt;Gas stove, gas and pot for making tea and for making Lipton soup powder mixed with those chicken noodle bag things for my sardine cans I will purchase in stores!&lt;/li&gt;
  &lt;li&gt;Batteries for long haul recharges&lt;/li&gt;
  &lt;li&gt;A smile!&lt;/li&gt;
  &lt;li&gt;Please forgive me if I forgot to mention something&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/bike_trip.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I will be putting all the stuff into bike bags and attaching to my bike (bike bags are sometimes referred to as panniers). My bicycle is black with leather seat and leather handle bars like a black stallion. Pretending you are riding on horse like some sort of cavalry officer from back in the day makes the miles go by way faster (like teleportation)- super fun!&lt;/p&gt;

&lt;p&gt;June 27th update:&lt;/p&gt;

&lt;p&gt;Listing the posts that I made on LinkedIn:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7195415478574485504-uMrj&quot;&gt;Intro - May 12, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7196522920435765248-BHhM&quot;&gt;Instagram post - May 15, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7197108987027234816-XFF5&quot;&gt;Far-Go and Go-Far - May 17, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7197590708604555265-vxVZ&quot;&gt;Reflections of my life in Winnipeg as I bike in America Part 1: Working the crowd. - May 18, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7197808880221917185-MNbU&quot;&gt;Reflections of my life in Winnipeg as I bike in America Part 2: Julieta - May 19, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7198183875221610496-T7yY&quot;&gt;Music interlude 1: CCR - May 20, 2024 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7198411741834219522-h8rr&quot;&gt;Reflections of my life in Winnipeg as I bike in America Part 3: Farmer Wisdom - May 20, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7198678947683852288-mHNW&quot;&gt;Reflections of my life in Winnipeg as I bike in America Part 4: Posture and neck movement watching - May 21, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7199038471783546880-30pj&quot;&gt;Reflections of my life in Winnipeg as I bike in America Part 5: Pepper trouble - May 22, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7199400699779059717-03iH&quot;&gt;Music interlude 2: Dancing - May 23, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7199789914190667780-GGGz&quot;&gt;Treatise 1: On Tobbaco - May 24, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7200134695873830912-QuXz&quot;&gt;Reflections of my life in Winnipeg as I bike in America Part 6: Canoeing with Rambo - May 25, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7200555959725801472-QD5x&quot;&gt;Music interlude 3: Rap music - May 26, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7200884901812854784-XPtR&quot;&gt;Treatise 2: On Bicycles - May 27, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7201237073045958657-6Fr2&quot;&gt;Treatise 3: On Dis Ease - May 28, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7201603061855707137-vltk&quot;&gt;Music interlude 4: War music - May 29, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7201951437751296000-BypB&quot;&gt;Treatise 4: On Green Rectangles - May 30, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7202305600620306433-2F6A&quot;&gt;Music interlude 5: Folk music - May 31, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7203404627801743360-eSSs&quot;&gt;Treatise 5: On War - June 3, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7204649762296324097-9rLM&quot;&gt;Conclusion 1: The Big Apple - June 7, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7204863068290359296-b_zM&quot;&gt;Conclusion 2: Saying goodbye to your best friend - June 7, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7205365741217325056-JsAm&quot;&gt;Music interlude 6: Giving thanks to God! - June 9, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7205553166396882944-JApr&quot;&gt;Conclusion 3: Hey Vinny - June 9, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7205996914608070656-PP3Q&quot;&gt;Conclusion 4: Q and A - June 10, 2024&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/posts/activity-7206304713393250307-kPjk&quot;&gt;Epilogue - June 11, 2024&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sat, 11 May 2024 19:00:01 -0500</pubDate>
        <link>http://localhost:4000/programming/2024/05/11/what-is-leadership/</link>
        <guid isPermaLink="true">http://localhost:4000/programming/2024/05/11/what-is-leadership/</guid>
        
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>The Bionicle Man</title>
        <description>&lt;p&gt;Hello dear reader! I am writing a new blog post as it would be amiss of me to miss adding a single post in the fading year of two thousand and twenty-three. During a small part of this busy busy year I have been preoccupied with my Garmin watch. My first watch model was a gift, and I initially thought it was a silly thing until I started using it.&lt;/p&gt;

&lt;p&gt;Garmin watches allow you to track your heart rate and GPS coordinates. This allows you to understand how hard you are exerting yourself and how far you went when running, cycling, swimming, etcetera. Garmin’s data sharing ability allows you to share this information with friends in order to see what new bike routes or challenges your friends have completed. This social aspect for me was the primary benefit of the watch, represented in thoughts such as: &lt;em&gt;“…wait what?… so and so biked 40km across a frozen lake on a fat bike?”&lt;/em&gt; and &lt;em&gt;“That’s an interesting cycling route. I will try that out tomorrow.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;You may be thinking: &lt;em&gt;“…ah another tracking mechanism that I don’t care for”&lt;/em&gt; or &lt;em&gt;“who has got time or knowledge to view and make use of these metrics?”&lt;/em&gt;. These are valid concerns, but the watch is what you make of it. I found the watch great for helping me become more mindful of my breath control during exercise, &lt;a href=&quot;https://www.youtube.com/watch?v=-6PDBVRkCKc&quot;&gt;training in specific heart rate zones&lt;/a&gt;, and changing up my exercise routes. The watches are now so advanced that they even come with a GPS map that can assist you in some of your longer &lt;a href=&quot;https://www.youtube.com/watch?v=wESL8G1pNIU&quot;&gt;routes&lt;/a&gt; - the technology is very advanced!&lt;/p&gt;

&lt;p&gt;Outside of social and exercise metrics, I wasn’t receiving too much information about my body. It wasn’t until I upgraded to the &lt;a href=&quot;https://www.garmin.com/en-CA/p/777730&quot;&gt;955 Solar Garmin watch&lt;/a&gt; that I started getting some additional metrics, such as &lt;a href=&quot;https://support.garmin.com/en-CA/?faq=WT9BmhjacO4ZpxbCc0EKn9&quot;&gt;stress level&lt;/a&gt;. At first I was very confused by this variable. How is it that I am sitting down, and this watch is claiming that I am stressed at a stress level of 80 out of 100… during a vacation where I do, and should feel calm?&lt;/p&gt;

&lt;p&gt;The stress metric is based on the heart rate and &lt;a href=&quot;https://medium.com/@altini_marco/the-ultimate-guide-to-heart-rate-variability-hrv-part-1-70a0a392fff4&quot;&gt;heart rate variability (hrv)&lt;/a&gt;. &lt;a href=&quot;https://www.garmin.com/en-US/garmin-technology/health-science/stress-tracking/&quot;&gt;Garmin&lt;/a&gt; scales the metrics from 0 to 100 in order of increasing stress. Below is a sample image of Garmin watch view of my stress during a morning where I had to go for an early flight. The calm, blue, below 25 stress occured when I was sleeping on the plane.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/watch_images/stress_view.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The stress metric was not immediately available from the watch’s resting face, and I had to press one of the side buttons to see the live data. I was so preoccupied with all of this that my friend nicknamed me the “Bionicle Man”. Half man, half robot - obsessed and hunched over my watch, looking at my metrics throughout all moments of the vacation.&lt;/p&gt;

&lt;p&gt;I decided to combine my old love for little programming projects with this new found curiosity in the dynamic stress metric. Fortunately, Garmin has a wonderful &lt;a href=&quot;https://developer.garmin.com/&quot;&gt;software development kit&lt;/a&gt; and &lt;a href=&quot;https://forums.garmin.com/developer/connect-iq/&quot;&gt;detailed forums&lt;/a&gt; that allows one to create their own watch faces and apps. I set out to create a stress monitoring watch!&lt;/p&gt;

&lt;p&gt;Unfortunately, when programming the watch face, &lt;a href=&quot;https://forums.garmin.com/developer/connect-iq/f/discussion/286382/stress-and-heart-rate-in-low-power-mode&quot;&gt;one cannot get access to a ‘live’ stress metric&lt;/a&gt;. Therefore, I used the next best thing for displaying live stress data, which was the live heart rate, as these two fields are correlated. I created a watch face with a graph that displayed the heart rate for the past three minutes, and that refreshed every second. I colour coded the heart rate in the graph in order to communicate to the user when their heart rate was exceeding a resting rate. Take a look at the resulting watch face:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/watch_images/cropped.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This graph allows you to look into the history of your heart rate; to see if your heart rate is accelerating or decelerating. For the gif above, I modified the code to allow for playback of live recorded data, to demonstrate how the watch face works (it’s not actually measuring the pulse of the table). For the represented data, I started in a sitting position, stood up did some jumping jacks, and at last I sat back down. My heart started at a resting rate in the 40’s/50’s and then went up to the 90’s, and then went back down to resting. The gif shows minute by minute updates. The live scroll can only last &lt;a href=&quot;https://forums.garmin.com/outdoor-recreation/outdoor-recreation/f/fenix-7-series/291191/hi-power-mode/1405564#1405564&quot;&gt;10 seconds at a time&lt;/a&gt;, when the watch is ‘active’, due to a strict battery saving restriction implemented by Garmin. During the live scroll, the graph shifts pixel by pixel, with the right most data containing the latest heart rate. If you want the latest graph data at any time, then you just have to ‘activate’ your watch using the wrist gesture (demonstrated below - as the watch is activated the screen turns on and the graph starts displaying live heart data and at the end the watch goes back to ‘sleep’ mode).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/watch_images/activate_watch.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Outside of my heart rate during jumping jacks, I observed many interesting things. Most captivating to me was my heart rate in a sitting and working context; one’s heart beats per minute can be in the 40’s or the 80’s, all while doing the exact same thing. One’s heart rate can drift away very easily from a steady rest rate to an elevated rate. Get carried away with a thought and next thing you know your heart rate went from the 40’s to the 70’s over the span of a minute. My heart rate is the calmest when I am deeply focused on a topic.&lt;/p&gt;

&lt;p&gt;These observations inspired the following two questions: How do I go from one heart rate to another, and how do I become more self aware of my body? I will not go into the details of the experiments that followed these questions, as this is not a health blog, but will mention that breathing techniques were very helpful. Having this new, easy access to my heart rate and its recent history, I was able to learn a lot and continue to learn much.&lt;/p&gt;

&lt;p&gt;This observation process worked in a feedback loop; as you read the data that is coming to you about your body, you modify your behaviour (like breathing) or environment, to see if you can register a different response from your body (lower heart rate for example). Yes, it is likely and most definitely true: there are many papers on this topic describing how effective it is or not for tracking stress. However, at the time I was too excited to start coding on this project and decided not to get bogged down in the academic papers, and their potentially uncertain conclusions. &lt;a href=&quot;https://www.youtube.com/watch?v=U2RpBeHJ33s&quot;&gt;Here&lt;/a&gt; is a video of another project dealing with this feedback mechanism.&lt;/p&gt;

&lt;p&gt;Programming the watch face was not a linear process. There were many uncertainties and unknowns along the way. For some of these unknowns I was not sure if the project could be finished, while for some others it was just a matter of debugging. Just as my heart rate went up and down, so did my concern for the project’s feasibility. The milestones of uncertainty in order of thought were the following:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;“Is programming a custom watch face even possible?”&lt;/li&gt;
  &lt;li&gt;“How do I get my environment setup?”&lt;/li&gt;
  &lt;li&gt;“How do I get a basic app going?”&lt;/li&gt;
  &lt;li&gt;“How do I print some text?”&lt;/li&gt;
  &lt;li&gt;“How do I add military time?”&lt;/li&gt;
  &lt;li&gt;“How do I transfer the custom watch face to my watch face?”&lt;/li&gt;
  &lt;li&gt;“What data structure should I use to store incoming heart rate data?”&lt;/li&gt;
  &lt;li&gt;“How do I add some tests to make sure the data structure is solid?”&lt;/li&gt;
  &lt;li&gt;“Can I get live stress data for every second?”&lt;/li&gt;
  &lt;li&gt;“What about heart data?”&lt;/li&gt;
  &lt;li&gt;“How do I deal with this ‘out of memory’ error?”&lt;/li&gt;
  &lt;li&gt;“Can I get the watch to display a live graph all the time, or only ten seconds at a time?”&lt;/li&gt;
  &lt;li&gt;“How do I make this graph look nice?”&lt;/li&gt;
  &lt;li&gt;“How do I add custom fonts to my watch face?”&lt;/li&gt;
  &lt;li&gt;“Which colours are available for my watch face and how do I assign them per increasing heart rate range?”&lt;/li&gt;
  &lt;li&gt;“How do I align everything on my watch to my taste?”&lt;/li&gt;
  &lt;li&gt;“What range of heart rate history should I add to my watch: 10, 5 or 3 minutes?”&lt;/li&gt;
  &lt;li&gt;“How do I want to organize and clean up the code to make a public repository to share with others?”&lt;/li&gt;
  &lt;li&gt;“What do I want to write in a blog post about the watch face to share with my friends?”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I wrote these series of questions down for the non-technical reader (you?) to bring them along with me on this journey. They weren’t really that “serious” or cliff hanger like. For any programming project this is a common process, especially for those projects which are do-it-yourself. I am sure you encounter these thought paths in the various projects you work on. For a more “serious” project I would spend more time in the planning phase - here I was just itching to start coding right away!&lt;/p&gt;

&lt;p&gt;For the technical reader, who is looking to create their own watch face, Garmin provides the ability to &lt;a href=&quot;https://medium.com/@JoshuaTheMiller/making-a-watchface-for-garmin-devices-8c3ce28cae08&quot;&gt;simulate your watch&lt;/a&gt; on the computer, so the development becomes very quick. The programming is straight forward, and Github &amp;amp; Garmin forums provide a plethora of examples to assist you in your project. The code I wrote can be found &lt;a href=&quot;https://github.com/mannyray/StressWatchFaces&quot;&gt;here&lt;/a&gt;. In regard to the code, I was most satisfied with the the &lt;a href=&quot;https://github.com/mannyray/StressWatchFace/blob/master/source/DataLinkedList.mc&quot;&gt;data structure&lt;/a&gt; used in storing the incoming heart rate. For this structure I used an array as a linked list for ease of handling fresh/stale data, with the added condition that the data entries in the array are equally spaced apart in time, and that there is logic to handle spotty/missing data. In addition, I implemented &lt;a href=&quot;https://github.com/mannyray/StressWatchFace/blob/master/source/DataLinkedListTest.mc&quot;&gt;tests for the data structure&lt;/a&gt; ,to give confidence in its usability, and therefore allow for reuse of the code in other applications with live sensor data. The &lt;a href=&quot;https://github.com/mannyray/StressWatchFace/blob/master/source/MannyrayWatchFaceView.mc#L360&quot;&gt;graph code here&lt;/a&gt; turned out very aesthetically pleasing. I was happy to conquer this, as one of my the concerns when coding up the watch was &lt;em&gt;“will it look good?”&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;There are lots of ways to customize the aesthetics of your watch face programmatically. Many other custom watch faces can be found in Garmin’s &lt;a href=&quot;https://apps.garmin.com/en-US/&quot;&gt;app store&lt;/a&gt;, which has all sorts of applications like &lt;a href=&quot;https://support.garmin.com/en-CA/?faq=mE1Jmwvzd84s9YuDqa3GB7&quot;&gt;finding your phone&lt;/a&gt;, &lt;a href=&quot;https://apps.garmin.com/en-US/apps/503a3260-3a13-43f4-96d5-081930a46078&quot;&gt;finding your car&lt;/a&gt;, &lt;a href=&quot;https://apps.garmin.com/en-US/apps/d5228d46-886e-49c0-a9bf-7eabd537a8d2&quot;&gt;displaying the latest bitcoin price&lt;/a&gt;, &lt;a href=&quot;https://apps.garmin.com/en-US/apps/00aa0c38-6eed-4822-89b0-01b4bdab2501&quot;&gt;marine autopilot&lt;/a&gt; and many a styled, refined, and exquisite watch faces to match one’s taste.&lt;/p&gt;

&lt;p&gt;Perhaps some version of my watch face design was already created and lost to time amongst the thousands of other watch faces in the app store. Regardless, I viewed the exercise of creating a watch face very useful to me, as it was a foray into creating other additional Garmin apps and watch faces.&lt;/p&gt;

&lt;p&gt;I am still interested in getting access to a ‘live’ stress metric for which I already have already some leading links (&lt;a href=&quot;https://github.com/vtrifonov-esfiddle/TestHrv&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://forums.garmin.com/developer/connect-iq/f/discussion/235708/getting-started-in-monkey-c-with-hrv&quot;&gt;2&lt;/a&gt;, &lt;a href=&quot;https://forums.garmin.com/developer/connect-iq/f/discussion/233162/hrv-rr-intervals-api&quot;&gt;3&lt;/a&gt;). Furthermore, the watch can be connected to custom mobile apps via Garmin’s &lt;a href=&quot;https://developer.garmin.com/connect-iq/core-topics/mobile-sdk-for-android/&quot;&gt;Mobile SDK&lt;/a&gt;, which opens up an entire universe of internet-of-things related based applications.&lt;/p&gt;

&lt;p&gt;Before we (the reader and I) get flooded with an ocean of ideas, I will mention that am glad I was able to focus on just one idea and see it through its completion. I am grateful to Garmin for providing me with a platform to able to create something like this. It is very easy nowadays to get amazing insight about one’s own body. However, I think it is important not to over index on all this. You don’t even need a watch to measure your heart rate.&lt;/p&gt;

&lt;p&gt;Remember not to hunch over and obsess; don’t become a “Bionicle Man”.&lt;/p&gt;
</description>
        <pubDate>Mon, 25 Dec 2023 06:00:00 -0600</pubDate>
        <link>http://localhost:4000/programming/2023/12/25/the-bionicle-man/</link>
        <guid isPermaLink="true">http://localhost:4000/programming/2023/12/25/the-bionicle-man/</guid>
        
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>Photo Gallery</title>
        <description>&lt;p&gt;I have just upgraded my photo gallery on this website with my own custom code. It is now more compact, more visually appealing and more fun for me to use. Check out the gallery &lt;a href=&quot;https://szonov.com/photos/&quot;&gt;here&lt;/a&gt; (or navigate to it via the navigation bar at the top of the page). Here is a comparison of the before and after of the photo gallery home page:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Original Photo Gallery Home Page&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;NEW Photo Gallery Home Page&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;img src=&quot;http://localhost:4000/assets/photo_gallery_old_home.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;img src=&quot;http://localhost:4000/assets/photo_gallery_new_home.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Here is the comparison of the gallery page:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Original Photo Gallery Home Page&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;NEW Photo Gallery Home Page&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;img src=&quot;http://localhost:4000/assets/photo_gallery_old_gallery_page.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;img src=&quot;http://localhost:4000/assets/photo_gallery_new_gallery_page.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The main difference is now I am using square thumbnails for more compact design and dates to give the various picture themes a timeline. The previous gallery was based on &lt;a href=&quot;https://github.com/aerobless/jekyll-photo-gallery&quot;&gt;jekyll-photo-gallery&lt;/a&gt; which is a photo gallery plugin for Jekyll based sites. I was not satisfied with the gallery lay out and wanted to learn how to create my own &lt;a href=&quot;https://jekyllrb.com/docs/plugins/generators/&quot;&gt;plugin for generating pages&lt;/a&gt;. This desire to explore generating pages was fueled by my recent discovery of vast amounts of generated sites that suck up Google search result spaces in order to suck up ad money. I was using the Google search bar as a basic division calculator (e.g. 123/11) and noticed the Google search result had a website telling me what a mixed fraction is and that the one I was searching for is one of them and oh look here is an advertisement. I am curious about mechanisms for generating such pages.&lt;/p&gt;

&lt;p&gt;Jekyll plugin framework makes this process fairly easy. The core logic is in a single &lt;a href=&quot;https://github.com/mannyray/jekyll-gallery-tags/blob/master/lib/gallery.rb&quot;&gt;ruby file&lt;/a&gt; with about 100 lines of code. The trickiest part was figuring out how Jekyll structures things but once that is figured out it is all just some basic for loops. The &lt;em&gt;annoying&lt;/em&gt; part was the related UI html/css work as I am not the biggest fan of it. Fortunately, due to the way Jekyll is structured, the UI work was minimal. I have added a detailed README so you could reproduce the results yourself if you so choose.&lt;/p&gt;

&lt;p&gt;In a gallery page, the plugin generator connects a square thumbnail to the original image. Together, all the square thumbnails create a compact design with lots of flashy thumbnails to see at once. This is similar to Instagram. I am relying on the collective knowledge of social-media-giants’ brightest behavioural phycologists to create the most beautiful eye candy. The purpose of a photo gallery is not just to show beautiful pictures but to also do it in a beautiful way. With my own code and keyboard I can fine tune this beauty.&lt;/p&gt;

&lt;p&gt;Focusing on presentation, beauty and aesthetics makes using the feature more exciting. By having a good way of presenting photos I am now excited to present even more photos. I am digging through my photos to see which additional ones I can upload. I am revisiting old memories. Furthermore, I am excited to get my camera out more often and improve my skills. There of course is the option of using a ready to go product like Instagram but where is the fun in that?&lt;/p&gt;

&lt;p&gt;With using someone else’s product, like Instagram, I am subject to how they want me to use it. I am not sure that I want to share my photos how they want to. Do I need the comments or likes? Do I need a user name? Do I need to view the commercials and algorithm curated posts? I am making this sounds more serious and spooky than it is. This isn’t about being ungovernable. I am interested in creating a tool as I specify it and seeing if it improves photography for me. This is a new frontier for me. Initially computer science and programming sucked me in due to being able to automate things so I can be lazy. Oftentimes I would spend more time writing the script to automate something than if I did the task myself. There was a satisfcation in having the computer do something for me. However, now I am excited and motivated by creating tools that can simplify some processes for me.&lt;/p&gt;

&lt;p&gt;Automating away laziness and simplifying processes sounds similar but there is a subtle difference. The former is reactionary while the latter is seeing where the newly created processes takes you next. I am not excited about the new photo gallery for automating something away but I am excited about how this will affect picture taking and sharing. In fact, with this new photo gallery I have more work to do than before as I now have to create the thumbnails.&lt;/p&gt;

&lt;p&gt;The photo gallery is far from ideal. Creating the thumbnails is a manual process. I don’t think there is a good way to automate it as only I can see the best ‘square’ of my picture. My current React based web browser tool is good but it can’t be used on the go like some phone app. This is something I will work on next.&lt;/p&gt;

&lt;p&gt;I have a few other projects in mind for my website. One’s website is a fun and good way to motivate yourself to explore and to showcase your explorations. It is a little bit like who has the coolest lawn decorations and maybe considered a little self centered (I confess). I hope to not ‘smell my own farts’ too much here. My goal is to create tools and new processes and see if it helps others. The more the better.&lt;/p&gt;
</description>
        <pubDate>Mon, 14 Nov 2022 18:00:00 -0600</pubDate>
        <link>http://localhost:4000/programming/2022/11/14/photo-gallery/</link>
        <guid isPermaLink="true">http://localhost:4000/programming/2022/11/14/photo-gallery/</guid>
        
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>Hot Peppers</title>
        <description>&lt;p&gt;I have created another video. Please go &lt;a href=&quot;https://www.youtube.com/watch?v=atcKhoJ6vCc&quot;&gt;here&lt;/a&gt; to see the video! This time it is about my experimentation with growing hot pepper plants indoors during the winter. These plants provided me with inspiration, joy and a taste of spring. I am very grateful for the experience and look forward to growing more stuff this upcoming winter. Summer is so short here that I am already preparing.&lt;/p&gt;

&lt;p&gt;I am starting to get the hang of making videos and will likely make more. I like the creativity involved and the opportunity to film non programming/technical stuff. Maybe next time I will make a video of me playing something on the piano. Stay tuned!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/hot_peppers.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 12 Jul 2022 19:00:00 -0500</pubDate>
        <link>http://localhost:4000/planting/2022/07/12/hot-peppers/</link>
        <guid isPermaLink="true">http://localhost:4000/planting/2022/07/12/hot-peppers/</guid>
        
        
        <category>planting</category>
        
      </item>
    
      <item>
        <title>Train Pictures</title>
        <description>&lt;p&gt;Hello dear reader!&lt;/p&gt;

&lt;p&gt;I have created a video of a project I have been working on for a while. In addition to being a dear reader you can now be a dear viewer too! I am very excited to share this with you.&lt;/p&gt;

&lt;p&gt;The video is about an automatic mechanism I created for capturing pictures of trains that go across a bridge in view of my apartment.&lt;/p&gt;

&lt;p&gt;Please go &lt;a href=&quot;https://www.youtube.com/watch?v=PlVF_0MooQE&quot;&gt;here&lt;/a&gt; to see the video!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/photos/winnipeg/IMG_6151.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the video I reference code that I have written as part of the project. The code is all discussed in my previous few blog posts:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://szonov.com/programming/2021/03/28/prototype/&quot;&gt;Prototype&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://szonov.com/programming/2021/08/15/controlling-dslr-using-android-app-raspberry-pi-and-gphoto2/&quot;&gt;Controlling DSLR using Android App, Raspberry Pi and gphoto2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://szonov.com/programming/2021/08/30/simple-online-realtime-tracking/&quot;&gt;Simple Online Realtime Tracker&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://szonov.com/programming/2021/11/05/detecting-coins-with-tensorflow/&quot;&gt;Detecting Coins with Tensorflow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://szonov.com/programming/2021/11/12/tensorflow-lite-speed-comparison/&quot;&gt;TensorFlow Lite Speed Comparison&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://szonov.com/programming/2022/01/03/react-image-tagging/&quot;&gt;React Image Tagging&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Wed, 02 Feb 2022 14:00:00 -0600</pubDate>
        <link>http://localhost:4000/programming/2022/02/02/train-pictures/</link>
        <guid isPermaLink="true">http://localhost:4000/programming/2022/02/02/train-pictures/</guid>
        
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>React Image Tagging</title>
        <description>&lt;p&gt;Hello dear reader!&lt;/p&gt;

&lt;p&gt;I created an in-browser image tagging tool using React and Python/Flask for the backend. The code is &lt;a href=&quot;https://github.com/mannyray/ImageTagger&quot;&gt;here&lt;/a&gt; along with a very detailed explanation in the README.md.&lt;/p&gt;

&lt;p&gt;The tool can be used for tagging many pictures with ease and then later for searching for the images via tags. Here are some sample screenshots:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/react/manitoba.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/react/search_2.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It was great to learn React. It was unsual and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;setState&lt;/code&gt; command at times seemed like the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;goto&lt;/code&gt; command that I always thought was improper to use. It took some time to adjust. I am grateful for the learning experience.&lt;/p&gt;
</description>
        <pubDate>Mon, 03 Jan 2022 11:00:00 -0600</pubDate>
        <link>http://localhost:4000/programming/2022/01/03/react-image-tagging/</link>
        <guid isPermaLink="true">http://localhost:4000/programming/2022/01/03/react-image-tagging/</guid>
        
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>TensorFlow Lite Speed Comparison</title>
        <description>&lt;p&gt;Hello dear reader!&lt;/p&gt;

&lt;p&gt;In this blog post we will investigate the speed of TensorFlow Lite models on various Android phones. TensorFlow Lite models are efficient and portable TensorFlow models that are used in machine learning applications on mobile devices like Android phones. In our case we will investigate the models from previous &lt;a href=&quot;https://szonov.com/programming/2021/11/05/detecting-coins-with-tensorflow/&quot;&gt;blog post&lt;/a&gt; where the models generated can detect coins (see screenshot of app in action below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/detecting_coins/result.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We will test two TensorFlow Lite models&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz&lt;/code&gt;&lt;/a&gt; pretrained Tensorflow 2 model which was trained further and converted to Tflite format in &lt;a href=&quot;https://github.com/mannyray/machineLearningStarter/blob/master/run_tensorflow/result_2.tflite&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;result_2.tflite&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md#mobile-models&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssd_mobilenet_v3_large_coco_2020_01_14.tar.gz&lt;/code&gt;&lt;/a&gt; pretrained Tensorflow 1 model which was trained further and converted to Tflite format in &lt;a href=&quot;https://github.com/mannyray/machineLearningStarter/blob/master/run_tensorflow/result.tflite&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;result.tflite&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The TensorFlow Lite models are tested on four different Android phones&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gsmarena.com/samsung_galaxy_j2_prime-8424.php&quot;&gt;Samsung Galaxy J2 Prime&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;1.5 GB Ram&lt;/li&gt;
      &lt;li&gt;2016 November Release&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gsmarena.com/motorola_one_action-9739.php&quot;&gt;Motorola One Action&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;4 GB Ram&lt;/li&gt;
      &lt;li&gt;2019 October Release&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gsmarena.com/google_pixel_4-9896.php&quot;&gt;Google Pixel 4&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;6 GB Ram&lt;/li&gt;
      &lt;li&gt;2019 October Release&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gsmarena.com/google_pixel_6-11037.php&quot;&gt;Google Pixel 6&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;8 GB Ram&lt;/li&gt;
      &lt;li&gt;2021 October Release&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-are-we-investigating&quot;&gt;What are we investigating?&lt;/h2&gt;

&lt;p&gt;We want to test the performance of the TensorFlow Lite models on the various Android phones. When running, the TensorFlow Lite model for input takes an image/snapshot from your cellphone’s camera and processes it and then outputs coordinates of where the detected objects are (in our case these objects are coins). The coordinates are used to plot bounding boxes around the objects as seen in screenshot from cellphone above.&lt;/p&gt;

&lt;p&gt;The input output happens over and over again to produce real time object detection. The time to process an image is measured in milliseconds and will be the measure of performance. The speed of processing depends on the TensorFlow model as well as the cellphone’s hardware. The faster the processing time the better the performance.&lt;/p&gt;

&lt;p&gt;We will investigate the two TensorFlow Lite models on the four phones to see how they perform. Finally, we want to investigate how to improve performance (i.e. processing time of an image with TensorFlow Lite model) for a fixed phone.&lt;/p&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;p&gt;Here we will briefly discuss the testing procedure used.&lt;/p&gt;

&lt;p&gt;The app for benchmark is from &lt;a href=&quot;https://github.com/mannyray/tfliteCustomApp/tree/benchmark_app&quot;&gt;tfliteCustomApp repository&lt;/a&gt;. To download locally:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/mannyray/tfliteCustomApp.git
cd tfliteCustomApp
git checkout benchmark_app
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Connect your Android phone to Android Studio and build and then run the app on your phone. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;result.tflite&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;result_2.tflite&lt;/code&gt; models can be placed in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tfliteCustomApp/app/src/main/assets&lt;/code&gt; directory by replacing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;detect.tflite&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;Make the appropriate modifications (if necessary) to the app in Android Studio and build and run. Usually we are either modifying the exact model used, modifying the thread count or adding a fan to the phone. Your context may vary.&lt;/p&gt;

&lt;p&gt;We set the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Clear log before launch&lt;/code&gt; to enabled in between each run. Run the android app to start logging results.&lt;/p&gt;

&lt;p&gt;You can filter for the exact processing times by filter specific &lt;a href=&quot;https://github.com/mannyray/tfliteCustomApp/blob/benchmark_app/app/src/main/java/org/tensorflow/lite/benchmark/detection/DetectorActivity.java#L225&quot;&gt;log line&lt;/a&gt; in Android Studio:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/tensorflow_lite_comparison/log.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The phone will be connected via USB cable to computer in order to collect logs. The phone will be detecting same objects for duration of your experiment runtime. Once finishing running your app you can copy the logs to a file for later processing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/tensorflow_lite_comparison/coins.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;We will start with looking at a specific cell phone model (Motorola One Action) and look at TensorFlow Lite model performance (the one trained with TensorFlow 1).&lt;/p&gt;

&lt;p&gt;We introduce a new parameter, thread count, that can be specified in the app during runtime. We experiment with varying thread count to determine the best performance possible for a given cell phone as the performance varies with thread count. In the figure below we run the app on the phone for thirty seconds. The first 10 seconds are cut off as that is when the application is loading or one is modifying the thread count via UI and so the performance is not stable.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/tensorflow_lite_comparison/Figure_1.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From the figure we can observe that the performance varies significantly when varying thread count. In the case of Motorola One Action running the TensorFlow 2 model, we have the red line, representing four thread count, is the best performing configuration with average processing time of 75ms. When we decrease OR increase thread count from four we increase the processing time. For low and high thread count (1 or 6) we observe the variance in processing time has increased compared to the optimal thread count of four.&lt;/p&gt;

&lt;p&gt;The previous image was specifically for Motorola One Action. What happens if we fix a TensorFlow Model and see how the various Android models perform? In the image below we plot the average one minute performance of each of the four cell phones on the TensorFlow 1 trained model while we vary the thread count.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/tensorflow_lite_comparison/Figure_2.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The old Samsung J2 prime is so slow that we cannot really see how well the others are performing. We can tell that the other phones are much better with the Pixels being the best and near each other performance wise. Let us see what happens when we normalize each of the curves in the figure above based on its maximum value (we normalize each curve by dividing each value by the largest value present in the curve which reduces the values to range of zero to one):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/tensorflow_lite_comparison/Figure_3.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The maximum values are either with one thread count or six as that is where the cell phone is the most slowest. For all the cell phones we can tell that the optimal performance is somewhere in between one and six threads. For Motorola One Action and Pixel 4 we observe that the optimal time is twice as good as the worst processing time (0.5 ratio). We observe that some phones are optimal at three threads while others at four (Pixel 4 vs Motorola One Action).&lt;/p&gt;

&lt;p&gt;With the previous figure we lose the ability to compare performance of a cell phone to another cell phone. Let us compare the fastest processing time (among all thread counts) for all cell phones against each model. The processing time is the average of one minute.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;TensorFlow 1 Model&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;TensorFlow 2 Model&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Samsung J2 Prime&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;517ms&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1174ms&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Motorola One Action&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;75ms&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;216ms&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Pixel 4&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;39ms&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;102ms&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Pixel 6&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;41ms&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;82ms&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We observe that the TensorFlow 2 model is slower for all the cell phones by at least a factor of two. We observe that Pixel 6 is the fastest but comparable to Pixel 4 when using TensorFlow 1 model ( 39ms to 41ms too close say one is better than the other).&lt;/p&gt;

&lt;p&gt;It is important to note that I do not mean that TensorFlow 1 is better than TensorFlow 2. When I use “TensorFlow N model” I am referring to with which version the model was trained with. The models themselves are different which you can confirm yourself by analyzing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.tflite&lt;/code&gt;s in &lt;a href=&quot;https://netron.app&quot;&gt;https://netron.app/&lt;/a&gt;. Due to TensorFlow versioning and based on the work &lt;a href=&quot;https://github.com/mannyray/machineLearningStarter&quot;&gt;here&lt;/a&gt; each model can only be created in that specific version of TensorFlow. The TensorFlow 1 model is more lightweight so it is expected that it performs better. Lightweight models can be found in TensorFlow 2 as well.&lt;/p&gt;

&lt;p&gt;When using some other model for your application you will have to consider its speed and understand that it can have a significant impact on performance as demonstrated in the table above. You will also have to to consider its ability of correctly detecting objects. I happened to train these specific models as these were the first ones I was able to setup the training data to TensorFlow Lite model pipeline successfully. TensorFlow pipeline can be a pain to setup.&lt;/p&gt;

&lt;p&gt;In addition, you will also have to consider the phone model. In our study the Samsung J2 Prime was the slowest phone and not competitive to others. What if we compare the Motorola One Action and Pixel 4 for the TensorFlow 1 Model. The former is 75ms while the latter is 39ms in average processing time. Is 75ms sufficient for your specific application?&lt;/p&gt;

&lt;p&gt;Let us assume your application is for tracking an object as it moves across a screen as it was in my &lt;a href=&quot;https://szonov.com/programming/2021/03/28/prototype/&quot;&gt;application&lt;/a&gt;. For the purpose of tracking I implemented a &lt;a href=&quot;https://github.com/mannyray/sort/&quot;&gt;Simple Online Realtime Tracking&lt;/a&gt; algorithm which is built on top of a Kalman Filter. In tracking applications, we need to make sure we have an accurate estimate of where our object is located. Our estimate of where the object is located depends on how frequently we obtain our measurements (among other things). For TensorFlow Lite models, the measurements are the bounding boxes around detected objects. In the video below observe the difference in lag of the bounding box as the thread count is increased and thus processing time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=xOvVSYoBjm8&quot; title=&quot;Tensorflow tflite app lag example&quot;&gt;&lt;img src=&quot;http://localhost:4000/assets/tensorflow_lite_comparison/coin_video.png&quot; alt=&quot;IMAGE ALT TEXT&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;An extreme example of tracking lag is if you are tracking an ant on the ground but you can only check once a minute. Last time you saw the ant, he was heading north. In a minute you expect him to be slightly north of where you last saw him. What if the ant turned west half way through the one minute? Now you have to search the ground for the ant. If you were checking once every 10 seconds you wouldn’t have to spend as much time looking for him even if he changed direction. Frequency of measurements is relevant to your ability to track. If we double the processing time from one phone model to the next (or from one TensorFlow model to the next) then this can have significant impact on you as your measurements are less frequent.&lt;/p&gt;

&lt;p&gt;In tracking, a cost function of accuracy is the Intersection Over Union measure. In the gif below consider the red is the true position of an object and blue box where you think it is. As the blue box more closely matches the location of the red, the IOU increases to a maximum of one. IOU is zero when there is no intersection.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/mannyray/sort/raw/master/python_implementation/example/assets/iou_example.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Some stills from the gif above:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;http://localhost:4000/assets/tensorflow_lite_comparison/target-160.png&quot; alt=&quot;img&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;http://localhost:4000/assets/tensorflow_lite_comparison/target-175.png&quot; alt=&quot;img&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;http://localhost:4000/assets/tensorflow_lite_comparison/target-188.png&quot; alt=&quot;img&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;http://localhost:4000/assets/tensorflow_lite_comparison/target-200.png&quot; alt=&quot;img&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In the image below I plot the IOU as I increase the time difference between arriving measurements in a simulation I ran &lt;a href=&quot;https://github.com/mannyray/sort/tree/master/python_implementation/example&quot;&gt;here&lt;/a&gt; (see example 13). The increase in time between arriving measurements could represent increased processing time for TensorFlow model. Too much lag can significantly reduce IOU (see black line and red line). The stills from gif can help get an idea of significance of lower IOU.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/tensorflow_lite_comparison/lag_analysis.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can always tune the Kalman filter or Simple Online Realtime Tracking algorithm to account for slower processing time but it can be annoying especially if you are not familiar with the algorithm. The faster the TensorFlow model the less you need to rely on your Kalman filter. For examples on tuning please see my work &lt;a href=&quot;https://github.com/mannyray/sort/tree/master/python_implementation/example&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;So far we have investigated processing time over a small period of time of thirty seconds. For tracking applications (or others) you may want to run your app for much longer. Let us see what happens when we increase runtime to fifteen minutes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/tensorflow_lite_comparison/Figure_4.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this case we are running the Pixel 4 with TensorFlow 2 model. The blue line is us running with a cooling fan while the orange is just the phone on its own. Images of the fan are below. It attaches to the phone directly.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;http://localhost:4000/assets/tensorflow_lite_comparison/cooler_1.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;http://localhost:4000/assets/tensorflow_lite_comparison/cooler_2.jpg&quot; alt=&quot;img&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;With the fan, the average processing time is around 90ms while without the average is at first 110ms and then increases to around 130ms. Without the fan, the phone gets really hot and overtime can shutdown from overheating.&lt;/p&gt;

&lt;p&gt;Adding a fan is one quick way to improve your application’s performance in the long run.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this blog post I analysed the performance of TensorFlow Lite models on various Android phones and provided real world numbers. I briefly discussed TensorFlow Lite models in the context of object tracking applications. Finally, I provided a good tip for long term TensorFlow Lite execution on the phone: using a fan.&lt;/p&gt;

&lt;p&gt;The applications of TensorFlow Lite models seem endless to me. I am very grateful that TensorFlow has provided the technology for developing these models and has provided sample code for which I can easily get started in creating these applications like in the case of the benchmark app. A recent application in which I used some of the information presented here can be found in a previous post about a &lt;a href=&quot;https://szonov.com/programming/2021/03/28/prototype/&quot;&gt;prototype&lt;/a&gt;. The gif of the prototype, that tracks oranges, is below. I hope the reader finds the information presented here useful for their experiments too.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://szonov.com/assets/prototype/out.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 12 Nov 2021 11:00:00 -0600</pubDate>
        <link>http://localhost:4000/programming/2021/11/12/tensorflow-lite-speed-comparison/</link>
        <guid isPermaLink="true">http://localhost:4000/programming/2021/11/12/tensorflow-lite-speed-comparison/</guid>
        
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>Detecting Coins with Tensorflow</title>
        <description>&lt;p&gt;Hello dear reader! I am very excited to share with you an Android app that can detect coins.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/detecting_coins/canada_coins.JPG&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/detecting_coins/result.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is a demonstration video of the app in action (click the image):&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.youtube.com/watch?v=w_0esVCmKus&quot; title=&quot;Video Title&quot;&gt;&lt;img src=&quot;http://img.youtube.com/vi/w_0esVCmKus/0.jpg&quot; alt=&quot;IMAGE ALT TEXT&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The repository for the code can be found &lt;a href=&quot;https://github.com/mannyray/machineLearningStarter&quot;&gt;here&lt;/a&gt;. The repository is meant to provide a series of scripts and instructions on how to get started with Tensorflow tflite models on the Android.&lt;/p&gt;
</description>
        <pubDate>Fri, 05 Nov 2021 17:00:00 -0500</pubDate>
        <link>http://localhost:4000/programming/2021/11/05/detecting-coins-with-tensorflow/</link>
        <guid isPermaLink="true">http://localhost:4000/programming/2021/11/05/detecting-coins-with-tensorflow/</guid>
        
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>Simple Online Realtime Tracker</title>
        <description>&lt;p&gt;Hello dear reader! I come with another post about code related to my &lt;a href=&quot;https://szonov.com/programming/2021/03/28/prototype/&quot;&gt;prototype&lt;/a&gt; project. The &lt;a href=&quot;https://szonov.com/programming/2021/08/15/controlling-dslr-using-android-app-raspberry-pi-and-gphoto2/&quot;&gt;last post&lt;/a&gt; showcased code where an Android App was able to control a DSLR camera via Bluetooth with a Raspberry Pi’s assistance. The Android App would send a capture image signal to the DSLR upon user’s manual input on the Android App. For object detection applications we want to automate this based on some object’s position.&lt;/p&gt;

&lt;p&gt;For my prototype, I would take a high resolution picture with the DSLR when the object was detected in a specific range. In the gif below, this range would be the yellow box and the objects are oranges.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/prototype/out.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When a new orange enters the range box (scope) the Android app would send a take picture signal. In the image below is when the signal is taken.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/prototype/video_capture.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And here is the picture taken by the DSLR (prototype setup so value of picture not important):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/prototype/IMG_6378.JPG&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;how-does-one-track-an-object-as-it-moves-around-the-view-of-the-androids-camera&quot;&gt;How does one track an object as it moves around the view of the Android’s camera?&lt;/h3&gt;

&lt;p&gt;For this I used the Simple Online Realtime Tracker (SORT) algorithm based on&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Bewley, Alex, et al. &quot;Simple online and realtime tracking.&quot; 2016 IEEE international conference on image processing (ICIP). IEEE, 2016.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The SORT combines the Kalman Filter and Hungarian algorithm together to keep track of the bounding boxes an image detecting machine learning model would output for each frame of some video. Every frame the machine learning model tells you that &lt;em&gt;here&lt;/em&gt; is an orange and &lt;em&gt;there&lt;/em&gt; is an orange and &lt;em&gt;over there&lt;/em&gt;. However, the image detecting machine learning model will not tell you that an orange in one frame at location X is the same orange at location Y. The SORT solves this problem.&lt;/p&gt;

&lt;p&gt;I have created a Python implementation and then translated it to Java since that is the language used in Android App development. Developing and testing the algorithm in Python is easier. The code can be found &lt;a href=&quot;https://github.com/mannyray/sort&quot;&gt;here&lt;/a&gt;. To get started with using the code go to&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/mannyray/sort/tree/master/python_implementation&quot;&gt;https://github.com/mannyray/sort/tree/master/python_implementation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/mannyray/sort/tree/master/java_implementation&quot;&gt;https://github.com/mannyray/sort/tree/master/java_implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For a detailed explanation of SORT algorithm and various ways you could tune its parameters please go to&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/mannyray/sort/tree/master/python_implementation/example&quot;&gt;https://github.com/mannyray/sort/tree/master/python_implementation/example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The motiviation for creating this standalone SORT library was&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Not wanting to learn/use someone else’s code and discover its limitations a little to late into the implementation&lt;/li&gt;
  &lt;li&gt;Freedom to easily customize and modify my code based on my project’s needs&lt;/li&gt;
  &lt;li&gt;I already had experience with the &lt;a href=&quot;https://github.com/mannyray/kalmanfilter&quot;&gt;Kalman Filter&lt;/a&gt; which is half of the SORT algorithm&lt;/li&gt;
  &lt;li&gt;Learn more about object detection type code&lt;/li&gt;
  &lt;li&gt;Create something someone else might use&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are a reader who is looking for a SORT algorihtm, I hope you find mine useful!&lt;/p&gt;
</description>
        <pubDate>Mon, 30 Aug 2021 17:00:00 -0500</pubDate>
        <link>http://localhost:4000/programming/2021/08/30/simple-online-realtime-tracking/</link>
        <guid isPermaLink="true">http://localhost:4000/programming/2021/08/30/simple-online-realtime-tracking/</guid>
        
        
        <category>programming</category>
        
      </item>
    
      <item>
        <title>Controlling DSLR using Android App, Raspberry Pi and gphoto2</title>
        <description>&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Hello dear reader! In this post I will be discussing how to control a DSLR camera using an Android app, Raspberry Pi, gphoto2 and some &lt;a href=&quot;https://github.com/mannyray/bluetoothPhoto&quot;&gt;code&lt;/a&gt;. I found this capability very useful for my endeavours and hope that you may as well for yours. I will go over motivations, how to setup, linked code and an example use case. If you are restless, at the end I have a gif of a time-lapse I was able to create with the help of this app.&lt;/p&gt;

&lt;p&gt;Here is a view of the deployed system:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/schema.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;gphoto2 is a free application that can be used to control a variety of &lt;a href=&quot;http://www.gphoto.org/proj/libgphoto2/support.php&quot;&gt;different cameras&lt;/a&gt; via commandline.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The Raspberry Pi can be replaced with another computer. Raspberry Pi is chosen as it is leightweight and cheap.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Android app is useful because it allows for wireless, remote control and a nice user interface.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;here-are-reasons-why-you-may-find-the-setup-useful&quot;&gt;Here are reasons why you may find the setup useful:&lt;/h2&gt;

&lt;h3 id=&quot;cameras-come-with-limitations&quot;&gt;Cameras come with limitations&lt;/h3&gt;

&lt;p&gt;In this blog I will be using the Canon M100.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/m100.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the case of the M100, the camera has a limitation on how long you can take time-lapse photos for. In the image below, three screenshots from the M100 configuration screen show that you can set to take photos every 2 to 4 seconds and up to 900 shots. Combining the two upper limits you can only takes a time-lapse spanning an hour in time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/camera_settings.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What if you want to take a time-lapse of a full day? With the use of a battery cable and this setup you can overcome such limitations. The Android acts as user interface enhancement instead of using gphoto2 commandline directly on the Raspberry Pi.&lt;/p&gt;

&lt;p&gt;M100 battery cable:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/wired_battery.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-camera-manufacturer-provided-android-app-is-not-sufficient&quot;&gt;The camera manufacturer provided Android app is not sufficient&lt;/h3&gt;

&lt;p&gt;The Android app provided here can be modified to monitor for events for which regular manufacture provided Android apps for your camera will not have. Want to take picture at a specific time? Take a picture only when you detect something in the field of focus of cell phone camera? Take a picture when you say “action”? You will have to code this yourself for which the Android app introduced in this post provides a great starting point.&lt;/p&gt;

&lt;p&gt;The programmatic freedom allows one to integrate your camera into a variety of applications.&lt;/p&gt;

&lt;h3 id=&quot;broken-camera-screen&quot;&gt;Broken camera screen&lt;/h3&gt;

&lt;p&gt;May sound like a stretch but I had this happen. Camera menu screen was busted. The app can be useful here for providing a basic screen interface.&lt;/p&gt;

&lt;h2 id=&quot;how-to-setup&quot;&gt;How to Setup&lt;/h2&gt;

&lt;h3 id=&quot;install-gphoto2-on-raspberry-pi&quot;&gt;Install gphoto2 on Raspberry Pi&lt;/h3&gt;

&lt;p&gt;You will have to install gphoto2 software. &lt;a href=&quot;https://github.com/gonzalo/gphoto2-updater&quot;&gt;This&lt;/a&gt; is a good repository on how to do this.&lt;/p&gt;

&lt;h3 id=&quot;connect-the-camera-to-the-raspberry-pi&quot;&gt;Connect the Camera to the Raspberry Pi&lt;/h3&gt;

&lt;p&gt;Connect the camera to the Raspberry Pi. In the case of the M100, I use the USB to mini USB B. Cable below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/wire.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The mini USB B port on the M100 is pictured below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/port.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The result will look something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/connection.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Turn your camera on and see if your camera has a setting to stay on without sleeping. On the M100 the setting looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/never_off.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the Raspberry Pi terminal run&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ps aux | grep gphoto
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kill&lt;/code&gt; on the non &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grep&lt;/code&gt; results. Next run&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gphoto2 --shell
&amp;gt;trigger-capture
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/initial_connection.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sometimes taking pictures will fail. In the image above, the failure was due to camera not focusing while in auto focus mode. For the second picture, I had to make sure I had better lighting.&lt;/p&gt;

&lt;p&gt;gphoto2 supports many &lt;a href=&quot;http://www.gphoto.org/proj/libgphoto2/support.php&quot;&gt;cameras&lt;/a&gt; to varying degrees. Your debug process and figuring out how to run your camera will thus vary. Be patient. By searching your error on a search engine you can usually find a thread on gphoto2’s repository or some stackoverflow post discussing a potential workaround. Here are some links that I found useful:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/gphoto/libgphoto2/issues/521&quot;&gt;maybe you are using an older version of gphoto2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/gphoto/libgphoto2/issues/30&quot;&gt;use debug mode&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/gphoto/libgphoto2/issues/197&quot;&gt;saving to sd card&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;…happy debugging!&lt;/p&gt;

&lt;h3 id=&quot;connecting-raspberry-pi-to-android-phone&quot;&gt;Connecting Raspberry Pi to Android Phone&lt;/h3&gt;

&lt;p&gt;Connect your phone and Raspberry Pi together. There are many instructions online on how to do this like &lt;a href=&quot;https://bluedot.readthedocs.io/en/latest/pairpiandroid.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;setting-up-the-bluetooth-server-on-raspberry-pi&quot;&gt;Setting up the Bluetooth server on Raspberry Pi&lt;/h3&gt;

&lt;p&gt;On the Raspberry Pi you will have to run&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo python server.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The python script can be found &lt;a href=&quot;https://github.com/mannyray/bluetoothPhoto/tree/master/bluetooth_server&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Apply a setting to your Raspberry Pi so it does not fall asleep and stays on.&lt;/p&gt;

&lt;p&gt;If you experience errors: make sure to install the required packages. For potential debugging when failing to advertise I found &lt;a href=&quot;https://stackoverflow.com/questions/37913796/bluetooth-error-no-advertisable-device&quot;&gt;this&lt;/a&gt; to be helpful.&lt;/p&gt;

&lt;h3 id=&quot;build-the-android-app&quot;&gt;Build the Android app&lt;/h3&gt;

&lt;p&gt;You must be able to build the Android app using Android Studio. This can get a bit involved for a first time builder. I recommend starting &lt;a href=&quot;https://developer.android.com/training/basics/firstapp&quot;&gt;here&lt;/a&gt; to get introduced to the basics.&lt;/p&gt;

&lt;p&gt;The location of the app code is &lt;a href=&quot;https://github.com/mannyray/bluetoothPhoto/tree/master/bluetooth_android_app/Bluetooth&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;intermission&quot;&gt;Intermission&lt;/h3&gt;

&lt;p&gt;If you followed instructions above you now have&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Bluetooth-paired the Android phone and Raspberry Pi&lt;/li&gt;
  &lt;li&gt;Launched the Bluetooth server on the Raspberry Pi&lt;/li&gt;
  &lt;li&gt;Connected your camera to the Raspberry Pi&lt;/li&gt;
  &lt;li&gt;Built and installed the application on your Android.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For illustration purposes I have setup the Raspberry Pi and camera.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/test_setup.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The camera is connected to the Raspberry Pi and the battery is cable powered.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/camera_test_setup.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The test subject image here is a map of Winnipeg:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/camera_test_subject.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Bluetooth server on the Raspberry Pi is running and waiting for a connection.&lt;/p&gt;

&lt;h3 id=&quot;running-the-android-phone-app&quot;&gt;Running the Android Phone App&lt;/h3&gt;

&lt;p&gt;Now let us launch the Android application which is called “Bluetooth”. The landing page will look like&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/landing_page.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is me holding the phone:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/view_of_phone.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After pressing connect you will be redicted to&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/choose_page.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And the Raspberry Pi will accept the connection:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/bluetooth_server_waiting.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When selecting “Single picture” and pressing the camera button you will see:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/picture_taken.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Bluetooth server will reflect these pictures taken:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/bluetooth_server_picture_taken.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The picture is also saved to the SD card of the camera.&lt;/p&gt;

&lt;p&gt;The other option here is to take a time-lapse:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/time_lapse.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The “Shutdown Signal” is for when you want to disconnect and quit the Bluetooth server. The app will then redict you back to the landing page.&lt;/p&gt;

&lt;h2 id=&quot;time-lapse-example&quot;&gt;Time-lapse example&lt;/h2&gt;

&lt;p&gt;Here is an example of the time-lapse feature. After the pictures were taken I combined them to form a gif. Earlier, I suggested that the M100’s time-lapse feature could be extended for a whole day. I was a bit lazy to do that for this post and did something that is quicker and would take up less memory:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/dlsr_control/sand.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now the skeptical reader might ask: “Why do you need an Android app to send a single command to start executing a time-lapse or even a single photograph? Why not modify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;server.py&lt;/code&gt; and run the code straight on the Raspberry Pi?”&lt;/p&gt;

&lt;p&gt;It is true. In such cases the phone is unnecessary. I could just run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gphoto2 --shell&lt;/code&gt; in the terminal. However, for applications where you need more advance features, the Android phone is very versatile. In this &lt;a href=&quot;https://szonov.com/programming/2021/03/28/prototype/&quot;&gt;post&lt;/a&gt; I describe such an advanced application that builds on top of the logic introduced here by using the phone’s camera and a Tensorflow model. This post is supposed to provide starting code and general introduction to combining the phone, camera and Raspberry Pi together. Happy hacking!&lt;/p&gt;
</description>
        <pubDate>Sun, 15 Aug 2021 07:00:00 -0500</pubDate>
        <link>http://localhost:4000/programming/2021/08/15/controlling-dslr-using-android-app-raspberry-pi-and-gphoto2/</link>
        <guid isPermaLink="true">http://localhost:4000/programming/2021/08/15/controlling-dslr-using-android-app-raspberry-pi-and-gphoto2/</guid>
        
        
        <category>programming</category>
        
      </item>
    
  </channel>
</rss>
